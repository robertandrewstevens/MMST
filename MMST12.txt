12
Cluster Analysis
12.1 Introduction
Cluster analysis, which is the most well-known example of unsupervised learning, is a very popular tool for analyzing unstructured multivariate data. Within the data-mining community, cluster analysis is also known as data segmentation, and within the machine-learning community, it is also known as class discovery. The methodology consists of various algo- rithms each of which seeks to organize a given data set into homogeneous subgroups, or “clusters.” There is no guarantee that more than one such group can be found; however, in any practical application, the underlying hypothesis is that the data form a heterogeneous set that should separate into natural groups familiar to the domain experts.
Clustering is a statistical tool for those who need to arrange large quan- tities of multivariate data into natural groups. For example, marketers use demographics and consumer profiles in an attempt to segment the market- place into small, homogeneous groups so that promotional campaigns may be carried out more efficiently; biologists divide organisms into hierarchical orders in order to describe the notion of biological diversity; financial man- agers categorize corporations into different types based upon relevant finan- cial characteristics; archaeologists group artifacts (e.g., broaches) found in
A.J. Izenman, Modern Multivariate Statistical Techniques, Springer Texts in Statistics, 407 DOI 10.1007/978-0-387-78189-1_12, © Springer Science+Business Media New York 2013
￼
408 12. Cluster Analysis
graves in order to understand movements of ancient peoples; physicians use medical records to cluster patients for treatment diagnosis; and audiologists use repeated utterances of specific words by different speakers to provide a basis for speaker recognition. There are many other similar examples,
Cluster analysis resembles methods for classifying items; yet the two data analytic methods are philosophically different from each other. First, in classification, it is known a priori how many classes or groups are present in the data and which items are members of which class or group; in cluster analysis, the number of classes is unknown and so is the membership of items into classes. Second, in classification, the objective is to classify new items (possibly in the form of a test set) into one of the given classes based upon experience obtained using a learning set of data; clustering falls more into the framework of exploratory data analysis, where no prior information is available regarding the class structure of the data. Third, classification deals almost exclusively with classifying observations, whereas clustering can be applied to clustering observations or variables or both observations and variables simultaneously, depending upon the context.
Methods for clustering items (either observations or variables) depend upon how similar (or dissimilar) the items are to each other. Similar items are treated as a homogeneous class or group, whereas dissimilar items form additional classes or groups. Much of the output of a cluster analysis is visual, with the results displayed as scatterplots, trees, dendrograms, sil- houette plots, and heatmaps.
12.1.1 What Is a Cluster?
This is a difficult question to answer mainly because there is no univer- sally accepted definition of exactly what constitutes a cluster. As a result, the various clustering methods usually do not produce identical or even similar solutions.
A cluster is generally thought of as a group of items (objects, points) in which each item is “close” (in some appropriate sense) to a central item of a cluster and that members of different clusters are “far away” from each other. In a sense, then, clusters can be viewed as “high-density regions” of some multidimensional space (Hartigan, 1975). Such a notion seems fine on the surface if clusters are to be thought of as convex elliptical regions.
However, it is not difficult to conceive of situations in which natural clus- terings of items do not follow this pattern. When the dimension of a space is large enough, these multidimensional items, plotted as points in that space, may congregate in clusters that curve and twist around each other; even if the various swarms of points are non-overlapping (which is unlikely), the oddly shaped configurations of points may be almost impossible to detect and identify using current techniques.
12.1.2 Example: Old Faithful Geyser Eruptions
The data for this example1 is a set of 107 bivariate observations, that were taken from a study of the eruptions of Old Faithful Geyser in Yel- lowstone National Park, Wyoming (Weisberg, 1985, p. 231). A geyser is a hot spring which occasionally becomes unstable and erupts hot water and steam into the air. Old Faithful Geyser is the most famous of all geysers and is an extremely popular tourist attraction. The variables measured are duration of eruption (X1) and waiting time until the next eruption (X2), both recorded in minutes, for all eruptions of Old Faithful Geyser between 6 a.m. and midnight, 1–8 August 1978. Prior to clustering, one could argue that there are two or three possible clusters in the data.
Because the two variables are measured on very different scales (the standard deviations of X1 and X2 being approximately 1 and 13, respec- tively), the derived clusters (using any clustering algorithm) are completely determined by X2, the interval between eruptions; the observations are di- vided into clusters by straight-line boundaries parallel to the horizontal axis. Without standardizing both variables, we cannot obtain a realistic partitioning of the data. So, for this example, we standardize the variables prior to clustering.
The results of this clustering study, where we set the number of clusters to be two or three for each method, are displayed in Figure 12.1. The most interesting result is that “perfect” clustering (according to our intuition) for both two and three clusters is accomplished only by the single-linkage, hierarchical agglomerative method (see first row of Figure 12.1). If we use the single-linkage results as the gold standard, we see that average-linkage and complete-linkage methods (second row), which produced the same re- sults for two and three clusters, had one incorrect allocation for two clusters and three incorrect allocations for three clusters. Although both of the non- hierarchical clustering methods, pam and K-means (third row), had perfect clustering for two clusters, they performed poorly for three clusters, where they both had 45 incorrect allocations.
12.2 Clustering Tasks
There are numerous ways of clustering a data set of n independent mea- surements on each of r correlated variables.
Clustering Observations: When we speak about “clustering,” we usu- ally think of clustering the n observations into groups, where the
1The data can be found in the file geyser on the book’s website.
12.2 Clustering Tasks 409
￼￼
￼410 12. Cluster Analysis
K=2 K=3 SL SL
90 90
80 80
70 70
60 60
50 50
40 40
1234512345
AL,CL AL,CL 90 90
80 80
70 70
60 60
50 50
40 40
1234512345
pam,K-means pam,K-means 90 90
80 80
70 70
60 60
50 50
40 40
1234512345 Durationoferuption(min) Durationoferuption(min)
FIGURE 12.1. Clustering results for Old Faithful Geyser data. The scat- terplots in the left column panels are solutions for K = 2 classes, with red and blue as the two cluster colors. The scatterplots in the right column pan- els are solutions for K = 3 classes, with red, green, and blue as the three cluster colors. The first row is the single-linkage (SL) solutions, the second row is both average-linkage (AL) and complete-linkage (CL) solutions, the third row is both pam and K-means solutions.
Intervalto nexteruption (m in)
Intervalto nexteruption (m in)
Intervalto nexteruption (m in)
number, K, of groups is unknown and has to be determined from the data. When analyzing microarray data, the observations may be, for example, tissue samples, disease types, or experimental conditions, and so this task is often referred to as “clustering samples.”
Clustering Variables: We may wish to partition the p variables into K distinct groups, where the number K is unknown and has to be de- termined from the data. A group may be determined by using only one variable; however, most clusters will be formed using several vari- ables. These clusters should be far enough apart (in some sense) that groupings are easily identifiable. Each cluster of variables may later be replaced by a single variable representative of that cluster. When analyzing microarray data, the variables are genes, and so we refer to this task as “gene clustering.”
Two-Way Clustering: Instead of clustering the variables or the observa- tions separately, it might in certain circumstances be more appropri- ate to cluster them both simultaneously. Two-way clustering is known by different names, such as “block clustering,” “direct clustering,” “biclustering,” or “co-clustering.” This goal is especially appropriate in microarray studies, where it is desired to cluster genes and tis- sue samples at the same time to show which subset of genes is most closely related to which subset of disease types.
NOTE: Because many of the clustering algorithms can be applied to ob- servations or variables (or both simultaneously), it will often be convenient in this chapter to use the generic word “item” when a distinction between observation or variable is unnecessary.
12.3 Hierarchical Clustering
There are two types of hierarchical clustering methods: agglomorative and divisive. Agglomerative clustering algorithms, often called “bottom- up” methods, start with each item being its own cluster; then, clusters are successively merged, until only a single cluster remains. Divisive clustering algorithms, often called “top-down” methods, do the opposite: they start with all items as members of a single cluster; then, that cluster is split into two separate clusters, and so on for every successive cluster, until each item is its own cluster. Most attention in the clustering literature has been on agglomerative methods; however, arguments have been made that divisive methods can provide more sophisticated and robust clusterings.
12.3 Hierarchical Clustering 411
￼
412 12. Cluster Analysis
12.3.1 Dendrogram
The end result of all hierarchical clustering methods is a dendrogram (i.e., hierarchical tree diagram), where the k-cluster solution is obtained by merging some of the clusters from the (k + 1)-cluster solution. The dendrogram may be drawn horizontal or vertical, depending upon user choice or software decision; both types give the same information. In this discussion, we assume a vertical dendrogram.
The dendrogram allows the user to read off the “height” of the linkage criterion at which items or clusters or both are combined together to form a new, larger cluster. Items that are similar to each other are combined at low heights, whereas items that are more dissimilar are combined higher up the dendrogram. Thus, it is the difference in heights that defines how close items are to each other. The greater the distance between heights at which clusters are combined, the more readily we can identify substantial structure in the data.
A partition of the data into a specified number of groups can be obtained by “cutting” the dendrogram at an appropriate height. If we draw a hor- izontal line on the dendrogram at a given height, then the number, K, of vertical lines cut by that horizontal line identifies a K-cluster solution; the intersection of the horizontal line and one of those K vertical lines then represents a cluster, and the items located at the end of all branches below that intersection constitute the members of the cluster.
Unlike the vertical distances, which are crucial in defining a solution, the horizontal distances between items are irrelevant; the software that draws a dendrogram is generally written so that the dendrogram can be easily interpreted. For large data sets, however, this goal becomes impossible.
12.3.2 Dissimilarity
The basic tool for hierarchical clustering is a measure of the dissimilarity or proximity (i.e., distance) of one item relative to another item. Which definition of distance is used in any given application is often a matter of subjective choice. Let xi,xj be any two points in Rr. Dissimilarities usually satisfy the following three properties:
1. d(xi,xj)≥0;
2. d(xi,xi)=0;
3. d(xj,xi) = d(xi,xj).
Such dissimilarities are termed metric or ultrametric according to whether they satisfy a fourth property. A metric dissimilarity satisfies
4a. d(xi,xj) ≤ d(xi,xk) + d(xk,xj),
and an ultrametric dissimilarity satisfies 4b. d(xi,xj) ≤ max{d(xi,xk),d(xj,xk)}.
Ultrametric dissimilarities can be displayed graphically by a dendrogram. There are several ways to define a dissimilarity, the most popular being
Euclidean distance and Manhattan city-block distance.
Let xi = (xi1,···,xir)τ and xj = (xj1,···,xjr)τ denote two points in
Rr. Then, these dissimilarity measures are defined as follows:
Euclidean: d(xi,xj) = [(xi −xj)τ(xi −xj)]1/2 = 􏰪􏰊rk=1(xik −xjk)2􏰫1/2.
Manhattan: d(xi,xj) = 􏰊rk=1 |xik −xjk|.
Minkowski: dm(xi,xj) = [􏰊rk=1 |xik −xjk|m]1/m.
In some applications, squared-Euclidean distance is used. Minkowski dis- tance includes as special cases Euclidean distance (m = 2) and Manhattan distance (m = 1).
These dissimilarity measures are all computed using raw data, not stan- dardized data. Standardization is usually recommended when the variabil- ity of the variables is quite different: a larger variability will have a more pronounced affect upon the clustering procedure than will a variable with relatively low variability.
Another dissimilarity measure uses correlation between observations as a basis for clustering:
1-correlation: d(xi,xj) = 1−ρij = 1−sij/sisj,
xlk, l = i,j. A relatively large absolute value of ρij suggests that xi and xj are “close” to each other, whereas a small correlation (ρij ≈ 0) suggests they are “far away” from each other. Thus, 1 − ρij is taken as a measure of “dissimilarity” between
xi andxj.
It is important to notice that for each of these dissimilarity measures, the
summations and averaging are computed over variables, not observations.
Given n observations, x1, . . . , xn ∈ Rr, the starting point of any hi- erarchical clustering procedure is to compute the pairwise dissimilarities between observations and then arrange them into a symmetric, (n × n) proximity matrix, D = (dij), where dij = d(xi,xj), with zeroes along the diagonal. If we are using correlation, the proximity matrix D = (dij ) is a symmetric, (r × r)-matrix with ijth dissimilarity dij = 1 − ρij .
where −1 ≤ ρij ≤ 1 is the correlation between the pair of observations xi and xj. Here, sij = 􏰊r (xik − x ̄i)(xjk − x ̄j), si = [􏰊r (xik − x ̄i)2]1/2,
􏰊r k=1
sj = [ (xjk − x ̄j)2]1/2, and x ̄l = r−1
􏰊r k=1
k=1
k=1
12.3 Hierarchical Clustering 413
414 12. Cluster Analysis
12.3.3 Agglomerative Nesting (agnes)
Table 12.1 lists the algorithm for agglomerative hierarchical clustering. The most popular of these clustering methods are referred to as single- linkage (or nearest-neighbor), complete-linkage (or farthest-neighbor), and a compromise between these two, average-linkage methods. Each of these clustering methods is defined by the way in which two clusters (which may be single items) are combined or “joined” to form a new, larger cluster. Single linkage uses a minimum-distance metric between clusters, complete linkage uses a greatest-distance metric, and average linkage computes the average distance between all pairs of items within the two different clusters, one item from each cluster. There is also a weighted version of average link- age, where the weights reflect the (possibly disparate) sizes of the clusters in question.
No one of these algorithms is uniformly best for all clustering prob- lems. Whereas the dendrograms from single-linkage and complete-linkage methods are invariant under monotone transformations of the pairwise dis- similarities, this property does not hold for the average-linkage method. Single-linkage often leads to long “chains” of clusters, joined by singleton points near each other, a result that does not have much appeal in practice, whereas complete-linkage tends to produce many small, compact clusters. Average linkage is dependent upon the size of the clusters, whereas sin- gle and complete linkage, which depend only upon the smallest or largest dissimilarity, respectively, do not.
12.3.4 A Worked Example
To understand agglomerative hierarchical clustering, we give a detailed analysis of a small example. Consider the following n = 8 bivariate points:
x1 = (1,3)τ,x2 = (2,4)τ,x3 = (1,5)τ,x4 = (5,5)τ, x5 = (5,7)τ,x6 = (4,9)τ,x7 = (2,8)τ,x8 = (3,10)τ.
A scatterplot of these points is given in Figure 12.2 (top-left panel). Using Euclidean distance, the upper-triangular portion of the symmetric, (8 × 8)- matrix D(1) is as follows:
12345678
1.414 0
￼￼0
2.000 1.414 0
4.472 5.657 6.708 5.099 3.162 4.243 5.385 4.000 4.000 4.472 5.000 3.162
7.280 6.083 5.385 5.385 3.606 1.414
1
2
3
4
50 2.236 3.162 60 2.236 70 2.236 80
0 2.000 4.123 4.243
￼
TABLE 12.1. Algorithm for agglomerative hierarchical clustering.
1. Input: Items L = {xi,i = 1,2,...,n}, n = initial number of clusters, each cluster of which contains one item.
2. Compute D = (dij ), the (n × n)-matrix of dissimilarities between the n clusters, where dij = d(xi,xj), i,j = 1,2,...,n.
3. Find the smallest dissimilarity, say, dIJ, in D = D(1). Merge clusters I and J to form a new cluster IJ.
4. Compute dissimilarities, dIJ,K, between the new cluster IJ and all other clusters K ̸= IJ. These dissimilarities depend upon which linkage method is used. For all clusters K ̸= I,J, we have the following linkage options:
Single linkage: dI J,K = min{dI ,K , dJ,K }.
Complete linkage: dI J,K = max{dI ,K , dJ,K }. 􏰊􏰊
Average linkage: dIJ,K = i∈IJ k∈K dik/(NIJNK),
where NIJ and NK are the numbers of items in clusters IJ and K, respec-
tively.
5. Form a new ((n−1)×(n−1))-matrix, D(2), by deleting rows and columns I and J and adding a new row and column IJ with dissimilarities computed from step 4.
6. Repeat steps 3, 4, and 5 a total of n−1 times. At the ith step, D(i) is a symmetric ((n−i+1)×(n−i+1))-matrix, i = 1,2,...,n. At the last step (i = n), D(n) = 0, and all items are merged together into a single cluster.
7. Output: List of which clusters are merged at each step, the value (or height) of the dissimilarity of each merge, and a dendrogram to summarize the clustering procedure.
Single Linkage. The smallest dissimilarity is d12 = d23 = d68 = 1.414. We choose to merge x2 and x3 to form the new cluster “23.” We next computenewdissimilarities,d23,K =min{d2K,d3K}forK=1,4,5,6,7,8. The (7 × 7)-matrix D(2) is given by the following:
8 7.280 5.385 5.385 3.606 1.414 0 2.236 80
The smallest dissimilarity is d1,23 = d68 = 1.414. We choose to merge x1 with the “23” cluster, producing a new cluster “123.” We next compute new dissimilarities, d123,K = min{d1,K , d23,K } for K = 4, 5, 6, 7, 8. The
1 23 4 5 6 7
0
1.414 0
4.472 3.162 0
5.657 4.243 2.000
1 23 4 5
6 7 6.708 5.099 5.000 3.162 4.123 4.243 2.236 3.162 0 2.236
12.3 Hierarchical Clustering 415
￼￼￼￼0
￼
416 12. Cluster Analysis
(6 × 6)-matrix D(3) is as follows:
123 4 5 6
￼123 4 5 6 7
0 3.162 4.243 5.000 0 2.000 4.123 0 2.236 0
7 8 3.162 5.385 4.243 5.385 3.162 3.606 2.236 1.414 0 2.236 80
￼￼The smallest dissimilarity is d68 = 1.414, and so we merge x6 and x8 to form the new cluster “68.” We compute new dissimilarities, d68,K = min{d6K , d8K } for K = 123, 4, 5, 7. This gives us the (5 × 5)-matrix D(4) ,
￼123 4 5 68 123 0 3.162 4.243 5.000 40 2.000 4.123 50 2.236
7 3.162 4.243 3.162 680 2.236 70
￼￼The smallest dissimilarity is d45 = 2.0, and so we merge x4 and x5 to form thenewcluster“45.”Wecomputenewdissimilarities,d45,K =min{d4K,d5K} for K = 123, 68, 7. This gives the (4 × 4)-matrix D(5) ,
￼123 45 68 123 0 3.162 5.000 45 0 2.236 680
7 3.162 4.243 2.236 70
￼￼The smallest dissimilarity is d45,68 = d68,7 = 2.236. We choose to merge the cluster “68” with x7 to produce the new cluster “678.” The new dis-
similarities, d678,K = min{d68,K , d7K } for D(6) ,
123 45 123 0 3.162 450
678
K = 123, 45, yield the matrix
678 3.162 2.236 0
￼￼￼The smallest dissimilarity is d45,678 = 2.236, so the next merge is the cluster “45” with the cluster “678.” The matrix D(7) is
￼123 123 0
45678
45678 3.162 0
￼￼The last merge is cluster “123” with cluster “45678,” and the merging dissimilarity is d123,45678 = 3.162. The dendrogram is displayed in the top- right panel of Figure 12.2.
Complete Linkage. Complete linkage uses the same idea as single linkage, but instead of taking the smallest dissimilarity as the distance measure between clusters, we take the largest such dissimilarity. From D(1) given
10
8
6
4
2
12.3 Hierarchical Clustering 417
￼￼￼8
6
￼￼￼7
￼5
￼￼34 2
1
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼12345 X1
￼￼￼￼FIGURE 12.2. Agglomerative hierarchical clustering for worked exam- ple using Euclidean distance. Top-left panel: Scatterplot of eight bivariate points. Other panels show dendrograms showing hierarchical clusters and value of Euclidean distance at merge points. Top-right panel: Single linkage. Bottom-left panel: Complete linkage. Bottom-right panel: Average linkage.
previously, we merge x2 and x3 to form the “23” cluster at height 1.414, as before. Using Euclidean distance (but omitting square-roots in the pre- sentation), the upper-triangular portion of the (7 × 7)-matrix D(2) is as follows:
￼7 8 5.099 7.280 4.000 6.083 4.243 5.385 3.162 3.606 2.236 1.414 0 2.236 80
The smallest dissimilarity is d68 = 1.414. We merge x6 and x8 to form a new cluster “68.” We compute new dissimilarities, d68,K = max{d6K , d8K }
1 23 4 5 6 7
0
2.0 0
4.472 5.657 4.000 4.472 0 2.000 0
123 4 5
6 6.708 5.385 4.123 2.236 0
￼￼2
3
6
8
Height 1234567
X2
1
4
5
7
Height Height 12345 1.52.02.53.0
11
22
33
44
55
66
88
77
418 12. Cluster Analysis
for K = 1,23,4,5,7. This gives us a (6 × 6)-matrix D(3),
￼1 23 4 5 0 2.000 4.472 5.657 0 4.000 4.472 0 2.000 0
68 7 7.280 5.099 6.083 4.000 4.123 4.243 2.236 3.162
￼1 23 4 5 68
The smallest dissimilarity is d1,23 = d45 = 2.0. We choose to merge the cluster “23” with x1 to form a new cluster “123.” We compute new dissim- ilarities, d123,K = max{d12,K , d3K } for K = 4, 5, 68, 7. This gives us a new (5 × 5)-matrix D(4),
0 2.236 70
￼￼123 4 5 68 123 0 4.472 5.657 7.280 4 0 2.000 5.385 50 3.606
7 5.099 4.243 3.162 680 2.236 70
￼￼The smallest dissimilarity is d45 = 2.0. We merge x4 and x5 to form a new cluster “45.” We compute dissimilarities, d45,K = max{d4K,d5K} for K = 123, 68, 7. This gives us a new (4 × 4)-matrix D(5),
123 45 68 7
￼￼123 45 68
0
5.657 7.280 0 5.385
5.099
4.243 0 2.236 70
￼The smallest dissimilarity is d68,7 = 2.236. We merge cluster “68” with x7 to form the new cluster “678.” New dissimilarities d678,K = max{d68,K , d7K } are computed for K = 123, 45 to give the new (3 × 3)-matrix D(6) ,
123 45 678 123 0 5.657 7.280 450 5.385 678 0
The last steps merge the clusters “45” and “678” with a merging value of d45,678 = 5.385, and then the clusters “123” and “45678” with a merging value of d123,45678 = 7.280. The dendrogram is displayed in the bottom-left panel of Figure 12.2.
Average Linkage. For average linkage, the distance between two clusters is found by computing the average dissimilarity of each item in the first
cluster to each item in the second cluster. √
￼￼￼We start with the matrix D(1). The smallest dissimilarity is d12 = 1.414, and so we merge x1 and x2 to form cluster “12.” We compute dis- similarities between the cluster “12” and all other points using the aver- age distance, d12,K = (d1K + d2K )/2, for K = 3, 4, 5, 6, 7, 8. For example,
￼2 =
12.3 Hierarchical Clustering 419
d12,3 = (d13 + d23)/2 = (√4 + √2)/2 = 1.707. The matrix D(2) is given by
￼￼￼8 6.681 5.385 5.385 3.606 1.414 2.236 80
The smallest dissimilarity is d68 = 1.414, and so we merge x6 and x8 to form the new cluster “68.” We compute dissimilarities between the cluster “68” and all other points and clusters using the average distance, d68,12 = (d16 + d26 + d18 + d28)/4 = 6.364, and d68,K = (d6K + d8K)/2, for K = 3,4,5,7. The matrix D(3) is
12 3 4 5 68 0 1.707 3.817 4.950 6.364 0 4.000 4.472 5.193
0
12 3 4 5
6 7
￼12 3 4 5 6 7
0
1.707 0
3.817 4.000 0
4.950 4.472 2.000
6.047 5.000 4.123 2.236
4.550 3.162 4, 243 3.162 2.236 0
0
0
￼￼7 4.550 3.162 4, 243 3.162 2.236 70
￼12 3 4 5 68
2.000 0
4.754 2.921 0
￼The smallest dissimilarity is d12,3 = 1.707, and so we merge x3 and the cluster “12” to form the new cluster “123.” We compute dissimilarities between the cluster “123” and all other points using the average distance, d123,68 = (d16 +d18 +d26 +d28 +d36 +d38)/6 = 5.974 and d123,K = (d1K + d2K + d3K )/3, for K = 4, 5, 7. This gives the matrix D(4):
￼123 4 5 68 7
￼0
3.878 0
4.791 2.000 0
5.974 4.087 4.754 4.243 2.921 3.162
123 4 5 68
The smallest dissimilarity is d45 = 2.0, and so we merge x4 and x5 to form the new cluster “45.” We compute dissimilarities between the cluster “45” and the other clusters as before. This gives the matrix D(5):
123 45 68 7
The smallest dissimilarity is d68,7 = 2.236, and so we merge x7 and the cluster “68” to form the new cluster “678.” This gives the matrix D(6):
0 2.236 70
￼￼￼123 45 68
0
4.334 0
5.974 4.087 3.837 3.702 0 2.236 70
￼￼￼123 45 678
123 0
45 678 4.334 5.345 0 3.792 0
￼
420 12. Cluster Analysis
The smallest dissimilarity is d45,678 = 3.782, and so we merge the two clusters “45” and “678” to form a new cluster “45678.” We merge the last two clusters and compute their dissimilarity d123,45678 = 4.940. The dendrogram is displayed in the bottom-right panel of Figure 12.2.
12.3.5 Divisive Analysis (diana)
The most-used divisive hierarchical clustering procedure is that proposed
by MacNaughton-Smith, Williams, Dale, and Mockett (1964).
The idea is that at each step, the items are divided into a “splinter” group (say, cluster A) and the “remainder” (say, cluster B). The splinter group is initiated by extracting that item that has the largest average dissimilarity from all other items in the data set; that item is set up as cluster A. Given this separation of the data into A and B, we next compute, for each item in cluster B, the following two quantities: (1) the average dissimilarity between that item and all other items in cluster B, and (2) the average dissimilarity between that item and all items in cluster A. Then, we compute the difference (1)–(2) for each item in B. If all differences are negative, we stop the algorithm. If any of these differences are positive (indicating that the item in B is closer on average to cluster A than to the other items in cluster B), we take the item in B with the largest positive difference, move it to A, and repeat the procedure. This algorithm provides a binary split of the data into two clusters A and B. This same procedure can then be used to obtain binary splits of each of the clusters A and B separately.
The dendrogram corresponding to divisive hierarchical clustering of the worked example is displayed in Figure 12.3. Compare the result with that of the various agglomerative hierarchical clustering options in Figure 12.2. The major difference we see is that x4 is now included in the cluster with items x1,x2, and x3, rather than in the other cluster.
12.3.6 Example: Primate Scapular Shapes
This example is a small part of a much larger study (Ashton, Oxnard, and Spence, 1965) on measurements of the scapulae (shoulder bones) from 30 genera covering most of the primate order. The data2 used in this example consist of measurements on the scapulae of five genera of adult primates
2The author thanks Charles Oxnard and Rebecca German for providing him with these data. The data can be found in the file primate.scapulae on the book’s website.
￼
12.3 Hierarchical Clustering 421
￼￼￼￼FIGURE 12.3. Divisive hierarchical clustering for the worked example using Euclidean distance.
representing Hominoidea; that is, gibbons (Hylobates), orangutans (Pongo), chimpanzees (Pan), gorillas (Gorilla), and man (Homo).
The measurements consist of indices and angles that are related to scapu- lar shape, but not to functional meaning. Other studies showed that gender differences for such measurements were not statistically significant, and so no attempt was made by the authors of the study to divide the specimens by gender. Interest centered upon determining the extent to which these scapular shape measurements could be useful in classifying living primates.
There are eight variables in this data set, of which the first five (AD.BD, AD.CD, EA.CD, Dx.CD, and SH.ACR) are indices and the last three (EAD, β, and γ) are angles. Of the 105 measurements on each variable, 16 were taken on Hylobates scapulae, 15 on Pongo scapulae, 20 on Pan scapulae, 14 on Gorilla scapulae, and 40 on Homo scapulae. The angle γ was not available for Homo and, thus, was not used in this example.
Agglomerative and divisive hierarchical methods were employed for clus- tering the scapulae data using all five indices and two of the angles (EAD and β). Figure 12.4 shows dendrograms from the single-linkage, average-linkage, and complete-linkage agglomerative hierarchical methods and the dendro- gram from the divisive hierarchical method. Although five clusters can be identified for each dendrogram, the single-linkage dendrogram, which shows long, stringy clusters, has a very different shape than do the other three dendrograms.
We can see that certain primates are separated from the others. In par- ticular, primates 6, 18, 20, 55, and 102 stand out in the agglomerative dendrograms, and primate 3 also stands out in the single-linkage dendro- gram.
2
3
6
8
Height 1234567
1
7
5
4
422 12. Cluster Analysis
￼￼SingleLinkage
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼AverageLinkage
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼CompleteLinkage
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼D ivisive
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼FIGURE 12.4. Dendrograms from hierarchical clustering of the primate scapulae data. Upper-left panel: single linkage. Upper-right panel: average linkage. Lower-left panel: complete linkage. Lower-right panel: divisive.
When an isolated observation appears high enough up in a dendrogram, it becomes a cluster of size one and, hence, plays the role of an outlier in the data. In fact, single linkage for five clusters produces three clusters each of size one (primates 3, 20, and 102), and average linkage produces one cluster of size one (primate 20). We see from Figure 12.4 that single-linkage and average-linkage clustering algorithms tend to have more isolated observa- tions than do either the complete-linkage or divisive clustering algorithms.
12.4 Nonhierarchical or Partitioning Methods
Nonhierarchical clustering methods (also known as partitioning methods) simply split the data items into a predetermined number K of groups or clusters, where there is no hierarchical relationship between the K-cluster solution and the (K + 1)-cluster solution; that is, the K-cluster solution is not the initial step for the (K + 1)-cluster solution. Given K, we seek to partition the data into K clusters so that the items within each cluster
￼Height Height 02468 0.51.01.52.0
11 55 127
49 10 13
711
15 10 615
932 13 35
11 43
16 47 833
14 61 32 62
35 34 43 36
47 41
33 46
61 50 62 37
39 38
57 42
53 45 56 64
65 52
52 59
59 49
63 58 37 53
45 56
49 65
64 63
58 60 38 39 42 57
60 48
40 51
54 40
55 44
34 54
36 12
41 55 46 16 508
44 14 482 516
24
33 28 17
74 21 105 26
92 24
102 22
66 30
100 27
72 29 93 31 104 19
71 70
98 81 85 23
96 25 78 66
91 100
84 72 89 93
97 104 82 78
99 83 103 84
67 89
69 97
68 91
77 79 81 99
95 103
73 71 87 98 88 85
75 96 86 75
101 86
90 101
76 80 79 67 83 69
80 68
94 94
17 88 30 82 21 90 26 76
24 87
29 77 31 95
20 28 22 74
27 105
25 92
18 73 19 18
70 102
23 20
Height Height 02468 012345
11 75
159 513 911
13 10 117
10 15
44 22
12 12
33 148
16 14 616 86
32 32 35 35
43 43
47 47
33 33
61 61 62 62
49 39
64 57 34 52
36 59
41 63 46 53
44 56
50 65
51 34 37 36 45 41
38 46 42 50
60 44 48 48 40 51
39 37 57 45 53 38
52 42
59 49
56 64
65 58
63 40
54 60
58 54
55 55 17 17
30 30
21 21 26 26 24 24
29 22 31 29 22 31
27 25 25 27
18 19
19 70
70 23
23 81
20 95
28 18
66 66 67 100 69 72
95 93 71 104 98 71
85 98
96 85
82 96 72 78 93 84
104 89
91 97 100 91
74 99 105 103
92 82 102 75
68 86
77 101
81 90 73 76
87 94
88 79 99 83
103 80
75 73
101 87 90 88
78 67
84 69
89 68
97 77
76 28 94 74
79 105
83 92
80 102
86 20
12.4 Nonhierarchical or Partitioning Methods 423
are similar to each other, whereas items from different clusters are quite dissimilar.
One sledgehammer method of nonhierarchical clustering would conceiv- ably involve as a first step the total enumeration of all possible groupings of the items. Then, using some optimizing criterion, the grouping that is cho- sen as “best” would be that partition that optimized the criterion. Clearly, for large data sets (e.g., microarray data used for gene clustering), such a method would rapidly become infeasible, requiring incredible amounts of computer time and storage. As a result, all available clustering techniques are iterative and work on only a very limited amount of enumeration. Thus, nonhierarchical clustering methods, which do not need to store large prox- imity matrices, are computationally more efficient than are hierarchical methods.
This category of clustering methods includes all of the partitioning meth- ods, (e.g., K-means, partitioning around medoids) and mode-searching (or bump-hunting) methods using parametric mixtures or nonparametric den- sity estimates.
12.4.1 K-Means Clustering (kmeans)
The popular K-means algorithm (MacQueen, 1967) is listed in Table 12.2. Because it is extremely efficient, it is often used for large-scale cluster- ing projects. Note that the K-means algorithm needs access to the original data.
The K-means algorithm starts either (1) by assigning items to one of K predetermined clusters and then computing the K cluster centroids, or (2) by pre-specifying the K cluster centroids. The pre-specified centroids may be randomly selected items or may be obtained by cutting a dendrogram at an appropriate height. Then, in an iterative fashion, the algorithm seeks to minimize ESS by reassigning items to clusters. The procedure stops when no further reassignment reduces the value of ESS.
The solution (a configuration of items into K clusters) will typically not be unique; the algorithm will only find a local minimum of ESS. It is recommended that the algorithm be run using different initial random assignments of the items to K clusters (or by randomly selecting K initial centroids) in order to find the lowest minimum of ESS and, hence, the best clustering solution based upon K clusters.
For the worked example, the K-means clustering solutions for K = 2, 3, 4 are listed in Table 12.3. For K = 2, ESS=23.5; for K = 3, ESS=8.67; and for K = 4, ESS=5.67. Note that, in general, we expect ESS to be a monotonically decreasing function of K, unless the solution for a given value of K turns out to be a local minimum.
424 12. Cluster Analysis
TABLE 12.2. Algorithm for K-means clustering.
1. Input: Items L = {xi,i = 1,2,...,n}, K = number of clusters.
2. Do one of the following:
• Form an initial random assignment of the items into K clusters and,
for cluster k, compute its current centroid, x ̄k, k = 1, 2, . . . , K.
• Pre-specify K cluster centroids, x ̄k, k = 1, 2, . . . , K.
3. Compute the squared-Euclidean distance of each item to its current cluster
￼centroid:
􏰏K 􏰏
(xi − x ̄k)τ (xi − x ̄k),
where x ̄k is the kth cluster centroid and c(i) is the cluster containing xi.
4. Reassign each item to its nearest cluster centroid so that ESS is reduced in magnitude. Update the cluster centroids after each reassignment.
5. Repeat steps 3 and 4 until no further reassignment of items takes place.
12.4.2 Partitioning Around Medoids (pam)
This clustering method (Vinod, 1969) is a modification of the K-medoids clustering algorithm. Although similar to K-means clustering, this algo- rithm searches for K “representative objects” (or medoids) — rather than the centroids — among the items in the data set, and a dissimilarity-based distance is used instead of squared-Euclidean distance. Because it min- imizes a sum of dissimilarities instead of a sum of (squared) Euclidean distances, the method is more robust to data anomolies such as outliers and missing values.
This algorithm starts with the proximity matrix D = (dij ), where dij = d(xi,xj), either given or computed from the data set, and an initial con- figuration of the items into K clusters. Using D, we find that item (called a representative object or medoid) within each cluster that minimizes the total dissimilarity to all other items within its cluster. In the K-medoids algorithm, the centroids of steps 2, 3, and 4 in the K-means algorithm (Table 12.2) are replaced by medoids, and the objective function ESS is re- placed by ESSmed. See Table 12.4 (steps 1, 2, 3, and 4a) for the K-medoids algorithm.
The partitioning around medoids (pam) modification of the K-medoids algorithm (Kaufman and Rousseeuw, 1990, Section 2.4) introduces a swap- ping strategy by which the medoid of each cluster is replaced by another item in that cluster, but only if such a swap reduces the value of the objec-
ESS =
k=1 c(i)=k
￼
12.4 Nonhierarchical or Partitioning Methods 425
TABLE 12.3. K -means clustering solutions (K = 2, 3, 4) for the worked example.
￼K k
2 1 2
3 1 2 3
4 1 2 3 4
Indexes 1,2,3,4 5,6,7,8
1,2,3 4,5 6,7,8
1,2,3 4,5 6,8 7
Centroid
(3.5, 8.5) (2.25, 4.25)
(1.33, 4.0) (5.0, 6.0) (3.0, 9.0)
(1.33, 4.0) (5.0, 6.0) (3.5, 9.5) (2.0, 8.0)
Within-Cluster SS 13.5
10.0
2.67 2.0 4.0
2.67 2.0 1.0 0.0
￼￼tive function. The pam algorithm is listed in Table 12.4 (steps 1, 2, 3, and 4b).
A disadvantage of both the K-medoids and the pam algorithms is that, although they run well on small data sets, they are not efficient enough to use for clustering large data sets.
12.4.3 Fuzzy Analysis (fanny)
The idea behind fuzzy clustering is that items to be clustered can be assigned probabilities of belonging to each of the K clusters (Kaufman and Rousseeuw, 1990, Section 4.4). Let uik denote the strength of membership of the ith item for the kth cluster. For the ith item, we require that the {uik} behave like probabilities; that is, uik ≥ 0, for all i and k = 1,2,...,K, and 􏰊Kk=1 uik = 1 for each i. This contrasts with the partitioning methods of kmeans or pam, where each item is assigned to one and only one cluster.
Given a proximity matrix D = (dij) and number of clusters K, the un- known membership strengths, {uik}, are found by minimizing the objective
function,
􏰏K 􏰊 􏰊 u 2 u 2 d i j i j ik jk
2􏰊 u2 . l lk
(12.1)
￼k=1
The objective function is minimized subject to the nonnegativity and unit
sum restrictions by using an iterative algorithm.
For the worked example, the solution (after 90 iterations) is given in Table 12.5, where the most likely cluster memberships are as follows: cluster 1: items 1, 2, 3; cluster 2: items 4, 5; cluster 3: items 6, 7, 8. The minimum of the objective function is 3.428.
426 12. Cluster Analysis
TABLE 12.4. Algorithms for K-medoid and partitioning-around-medoids clustering.
1. Input: proximity matrix D = (dij); K = number of clusters.
2. Form an initial assignment of the items into K clusters.
3. Locate the medoid for each cluster. The medoid of the kth cluster is defined as that item in the kth cluster that minimizes the total dissimilarity to all other items within that cluster, k = 1, 2, . . . , K.
4a. For K-medoids clustering:
• Forthekthcluster,reassigntheikthitemtoitsnearestclustermedoid
so that the objective function,
￼ESSmed =
is reduced in magnitude, where c(i) is the cluster containing the ith
item.
• Repeatstep3andthereassignmentstepuntilnofurtherreassignment of items takes place.
4b. For partitioning-around-medoids clustering:
• For each cluster, swap the medoid with the non-medoid item that
gives the largest reduction in ESSmed.
• Repeat the swapping process over all clusters until no further reduc-
tion in ESSmed takes place.
12.4.4 Silhouette Plot
A useful feature of partitioning methods based upon the proximity matrix D (e.g., kmeans, pam, and fanny) is that the resulting partition of the data can be graphically displayed in the form of a silhouette plot (Rousseeuw, 1987).
Suppose we are given a particular clustering, CK, of the data into K clusters. Let c(i) denote the cluster containing the ith item. Let ai be the average dissimilarity of that ith item to all other members of the same cluster c(i). Also, let c be some cluster other than c(i), and let d(i,c) be the average dissimilarity of the ith item to all members of c. Compute d(i, c) for all clusters c other than c(i). Let bi = minc̸=c(i) d(i,c). If bi = d(i,C), then, cluster C is called the neighbor of data point i and is regarded as the second-best cluster for the ith item.
􏰏K 􏰏 k=1 c(i)=k
diik,
￼
12.4 Nonhierarchical or Partitioning Methods 427
TABLE 12.5. Fuzzy clustering for the worked example with K = 3. The boldfaced entries show the most probable cluster memberships for each item.
￼Cluster k i123
￼￼1 0.799 0.117
2 0.828 0.107
3 0.735 0.146
4 0.116 0.790
5 0.102 0.715
6 0.072 0.146
7 0.196 0.239
8 0.064 0.097
0.083 0.065 0.119 0.094 0.183 0.782 0.565 0.839
￼The ith silhouette value (or width) is given by si(CK)=siK = bi−ai ,
so that −1 ≤ siK ≤ 1. Large positive values of siK (i.e., ai ≈ 0) indicate that the ith item is well-clustered, large negative values of siK (i.e., bi ≈ 0) indicate poor clustering, and siK ≈ 0 (i.e., ai ≈ bi) indicates that the ith item lies between two clusters. If maxi{siK} < 0.25, this indicates either that there are no definable clusters in the data or that, even if there are, the clustering procedure has not found it. Negative silhouette widths tend to attract attention: the items corresponding to these negative values are considered to be borderline allocations; they are neither well-clustered nor are they assigned by the clustering process to an alternative cluster.
A silhouette plot is a bar plot of all the {siK} after they are ranked in decreasing order, where the length of the ith bar is siK. For the worked example, where we used the pam clustering method with K = 3 clusters, the silhouette plot is displayed in Figure 12.5.
The average silhouette width, s ̄K, is the average of all the {siK}. For the worked example with K = 3, the overall average silhouette width is s ̄3 = 0.51. (For K = 2, s ̄2 = 0.44, and for K = 4, s ̄4 = 0.41.) The statistic s ̄K has been found to be a very useful indicator of the merit of the clustering CK. The average silhouette width has also been used to choose the value of K by finding K to maximize s ̄K .
As a clustering diagnostic, Kaufman and Rousseeuw defined the silhou- ette coefficient, SC = maxK{s ̄K}, and gave subjective interpretations of its value:
￼max{ai, bi}
(12.2)
428 12. Cluster Analysis
￼￼1 2 3
4 5
8 6 7
FIGURE 12.5. Silhouette plot for the worked example using the partition- ing around medoids (pam) clustering method with K = 3 clusters.
￼￼￼￼￼￼￼￼0.0 0.2 0.4 0.6 0.8 1.0 Silhouettewidth
Averagesilhouettewidth:0.51
￼SC
0.71–1.00 0.51–0.70 0.26–0.50 ≤ 0.25
Interpretation
A strong structure has been found
A reasonable structure has been found The structure is weak and could be artificial No substantial structure has been found
￼￼12.4.5 Example: Landsat Satellite Image Data
Since 1972, Landsat satellites orbiting the Earth have used a combina- tion of scanning geometry, satellite orbit, and Earth rotation to collect high-resolution multispectral digital information for detecting and moni- toring different types of land surface cover characteristics. The Landsat data in this example were generated from a Landsat Multispectral Scanner (MSS) image database used in the European Statlog Project for assessing machine-learning methods.3 The following description of the data is taken from the Statlog website:
One frame of Landsat MSS imagery consists of four digital im- ages of the same scene in different spectral bands. Two of these are in the visible region (corresponding approximately to green and red regions of the visible spectrum) and two are in the (near) infrared. Each pixel is an 8-bit word, with 0 correspond-
3These data, which are available in the file satimage at the book’s website, can also be downloaded from http://www.niaad.liacc.up.pt/old/statlog/. For information on the Landsat satellites, see http://edc.usgs.gov/guides/landsat mss.html.
￼￼
12.4 Nonhierarchical or Partitioning Methods 429
TABLE 12.6. Comparison of results of different clustering algorithms ap- plied to the Landsat image data. The data consist of six groups of 4,435 ob- servations measured on 36 variables. Prior to clustering, all variables were standardized. The six derived clusters are designated A–F . The agglomera- tive hierarchical clustering methods are single-linkage (SL), average-linkage (AL), and complete-linkage (CL), and the nonhierarchical methods are K- means and partitioning around mediods (pam). Each column in this ta- ble gives the cluster sizes distributed among the six clusters, ordered from largest cluster (A) to smallest cluster (F).
￼Cluster SL AL CL
A 4,428 2,203 1,717
B 2 1,764 1,348
C 1370885
D 157266
E 123162
F 11857
K -Means pam 1,420 999 1,134 937 763790 694708 242613 182388
￼￼ing to black and 255 to white. The spatial resolution of a pixel is about 80m×80m. Each image contains 2,340×3,380 such pixels. The data set is a (tiny) sub-area of a scene, consisting of 82×100 pixels. Each line of the data corresponds to a 3×3 square neigh- borhood of pixels completely contained within the 82×100 sub- area. Each line contains the pixel values in the four spectral bands of each of the 9 pixels in the 3×3 neighborhood.
The 36 variables are arranged in groups of four spectral bands (1, 2, 3, 4) covering each pixel of the 3×3 neighborhood (top-left (TL), top-center (TC), top-right (TR); center-left (CL), center-center (CC), center-right (CR); bottom-left (BL), bottom-center (BC), bottom-right (BR)). The center pixel (CC) of each of 4,435 neighborhoods is classified into one of six classes: 1. red soil (1,072), 2. cotton crop (479), 3. gray soil (961), 4. damp gray soil (415), 5. soil with vegetation stubble (470), and 7. very damp gray soil (1,038). There is no class 6. Although we do not use these classifications in the clustering algorithms, we can compare our results with the true classifications.
The results of five clustering methods (we specified six clusters for each method) are given in Table 12.6. We see that of the agglomerative hierarchi- cal clustering methods, single-linkage (SL) puts almost all the observations into a single cluster, whereas average-linkage (AL) and complete-linkage (CL) are somewhat better at distributing the observations among the six clusters. K-means is better still, but pam is closest to the true configuration of the data. The pam silhouette plot for six clusters is given in Figure 12.6 and the average silhouette width is 0.32.
￼430 12. Cluster Analysis
-0.2 0.0 0.2 0.4 0.6 0.8 1.0 Silhouettewidth
Averagesilhouettewidth:0.32
FIGURE 12.6. Silhouette plot for the Landsat image example using the partitioning around medoids (pam) clustering method with K = 6 clusters.
The largest four eigenvalues of the (36 × 36) correlation matrix of the Landsat data are 18.68, 14.08, 1.61, and 0.91, respectively. Kaiser’s rule says that we should retain only those PCs whose eigenvalues are greater than unity; in this case, we retain the first three PCs. In Figure 12.7, we display a scatterplot of the first two PC scores of the Landsat data. The six clusters of points (corresponding to Table 12.6) found using the pam algorithm are each identified by their color. The scatterplot of the PC scores appears to be wedge-shaped, with three primary “rods.” The “bottom” rod is divided into three distinct bands, consisting of clusters A (dark blue), C (red), and B (green); the “middle” rod is similarly divided up into three distinct bands of clusters D (orange), E (light blue), and some B (green); and the “top” rod only consists of cluster F (brown). There are also many points in the scatterplot that fall between the rods.
The picture becomes more interpretable if we look at a 3D scatterplot of the first three PC scores (not shown here), especially if we use a rota- tion/spin operation as is available in S–Plus or R. Rotating the 3D plot shows a tripod-like structure, with the top of the tripod being cluster B and the three rods being the three legs of the tripod. We can compute a confusion table, Table 12.7, which details how many neighborhoods from each class are allocated to the various clusters. From Table 12.7, we see that one leg consists of clusters of primarily different types of gray soil (A, C, and B); the second leg consists of clusters of primarily red soil (D and E); and the third leg consists of a cluster of cotton crop (F). Image neighborhoods classified by Landsat as soil with vegetation stubble appear mostly within clusters B and E.
￼15
10
5
0
-5
12.5 Self-Organizing Maps (SOMs) 431
2ndPrincipalComponent
-7-238 1stPrincipalComponent
FIGURE 12.7. Scatterplot of first two principal components of the Land- sat image data, with points colored to identify the clusters found in the data. The six derived clusters are A. dark blue; B. green; C. red; D. orange; E. light blue; F. brown.
12.5 Self-Organizing Maps (SOMs)
The self-organizing map (SOM) algorithm (Kohonen, 1982) has its roots in artificial neural networks and has also been likened to methods such as multidimensional scaling (MDS; see Chapter 14) and K-means clustering. It is also referred to as a Kohonen self-organizing feature map. The original motivation for SOMs was expressed in terms of an artificial neural network
TABLE 12.7. The confusion table showing results of the pam clustering algorithm applied to the Landsat image data. The six derived clusters are designated A–F. The entry in the ith row and jth column shows the number of neighborhoods classified by Landsat into the ith image-type and allocated to the jth cluster.
Class A B 122 0 2 0 1 3 883 1 4 78 18 50 249 715 668
Total 999 937
C D E F Total
11 10 63
307 48 351 790
651 388 0 1,072 8 72 388 479 14 0 0 961 4 7 0 415 31 142 0 470 0 4 0 1,038 708 613 388 4,435
432 12. Cluster Analysis
￼RectangularSOM grid
HexagonalSOMgrid
￼￼￼￼￼FIGURE 12.8. Displays of 10×15 rectangular and hexagonal SOM grids. for modeling the human brain, and much of the literature still uses the
image of neurons in describing the building blocks of a SOM.
SOMs have been applied to clustering problems in fields as diverse as geographical information systems, bioinformatics, medical research, physi- cal anthropology, natural language processing, document retrieval systems, and ecology. Its primary use is in reducing high-dimensional data to a lower- dimensional nonlinear manifold, usually two or three dimensions, and in displaying graphically the results of such data reduction. In a SOM, the aim is to map the projected data to discrete interconnected nodes, where each node represents a grouping or cluster of relatively homogeneous points.
12.5.1 The SOM Algorithm
Two versions of the SOM algorithm are available: an “on-line” version, in which items are presented to the algorithm in sequential fashion (one at a time, possibly in random order), and a “batch” version, in which all the data are presented together at one time. Both algorithms are due to Kohonen.
The end product of the SOM algorithm (after a large number of iteration steps) is a graphical image called a SOM plot. The SOM plot is displayed in output space and consists of a grid (or network) of a large number of inter- connected nodes (or artificial neurons). In two dimensions, the nodes are typically arranged as a square, rectangular, or hexagonal grid. See Figure 12.8. For visualization reasons, an hexagonal grid is preferred.
In a two-dimensional rectangular grid, for example, the set of rows is K1 = {1,2,...,K1} and the set of columns is K2 = {1,2,...,K2}, where K1 (the height) and K2 (the width) are chosen by the user. Then, a node is defined by its coordinates, (l1, l2) ∈ K1 × K2. The total number of nodes, K = K1K2, is usually chosen by trial and error, initially much larger than the suspected number of clusters in the data. After an initial SOM analysis, one can reconfigure the SOM by reducing the number of row and column nodes. It will be convenient to map the collection of nodes into an ordered
12.5 Self-Organizing Maps (SOMs) 433
sequence, so that the node (l1,l2) ∈ K1 ×K2 is relabeled as the index k = (l1 − 1)K2 + l2 ∈ K, where K = {1, 2, . . . , K }.
The SOM algorithm has much in common with K-means clustering. In K-means clustering, items assigned to a particular cluster are averaged to obtain a “cluster centroid” (or “representative” of that cluster), which is subsequently updated. With this in mind, we associate with the kth node in a SOM plot a representative in input space, mk ∈ Rr, k ∈ K. Represen- tatives have also been called synaptic weight vectors, prototypes, codebook vectors, reference vectors, and model vectors. It is usual to initialize the process by setting the components of mk, k ∈ K, to be random numbers.
12.5.2 On-line Versions
At the first step of the on-line SOM algorithm, we set up the map size (i.e., select K1 and K2) and initialize all representatives {mk} so that they each consist of random values.
At each subsequent step of the algorithm, an item x is randomly selected from the data set and standardized. In this way, no component variable has undue influence on the results just because it has a large variance or absolute value. We then present x (which is now standardized) to the SOM algorithm.
We compute the Euclidean distance between x and each representative and find that node whose representative yields the smallest distance to x. If
k∗ = arg min{∥ x − mk ∥}, (12.3) k
where ∥ · ∥ denotes Euclidean norm, then the representative mk∗ is declared the “winner,” and k∗ is referred to as the best-matching unit (BMU) or winning node for the input vector x.
Next, we look at those nodes that are “neighbors” of the winning node. Anodek′ ∈Kisdefinedtobeagridneighborofthenodek∈Kifthe Euclidean distance between mk and mk′ is smaller than a given threshold c. The set of nodes, Nc(k∗), which are grid neighbors of the winning node k∗, is called the neighborhood set for that node. We then update the rep- resentatives corresponding to each grid neighbor of the winning node k∗ (including k∗ itself) so that each mk, k ∈ Nc(k∗), is closer to x; the simplest way of doing this is to use the uniformly weighted update formula,
mk ←mk +α(x−mk), k∈Nc(k∗), (12.4)
w h e r e 0 < α < 1 i s a l e a r n i n g - r a t e f a c t o r . F o r k ∈/ N c ( k ∗ ) , w e s e t α = 0 , s o that mk, k ∈/ Nc(k∗), remains unchanged. This process, which is repeated a large number of times, runs through the collection of input vectors one at
434 12. Cluster Analysis
a time. A useful “rule of thumb” is to run the algorithm steps for at least 500 times the number of nodes (Kohonen, 2001, p. 112).
A “distance-weighted” version of (12.4) is probably the more popular strategy,
mk ← mk + αhk(x − mk), k ∈ Nc(k∗), (12.5)
where the neighborhood function h depends upon how close the neighboring representatives are to mk∗ . Those representatives that are neighbors of mk∗ are adjusted, but not by as much as is mk∗ ; the further a neighbor is from mk∗ , the less of an adjustment is made. The h-function takes the value one when the distance is zero and becomes progressively smaller as the distances become larger. For k ∈/ Nc(k∗), we set hk = 0. The most-popular h-function is the multivariate Gaussian kernel function,
􏰇 ∥mk−mk∗ ∥2􏰢
hk = exp − 2σ2 I[k∈Nc(k∗)], (12.6)
where σ > 0 is the neighborhood radius.
Values of c, α, and σ are provided by the user but may change during the sequential process. In the on-line process, c is shrunk during the first 1,000 or so observations from, say, an initial value of C (chosen by the user) to 1. If we take the threshold value c to be so small that each neighborhood contains only a single point, then we lose the dependencies between representatives, which would be independently updated, and the SOM algorithm reduces to an on-line version of K-means clustering, where K is the total number of nodes. The value of α decreases from a large initial value of just less than 1 to a value slightly greater than zero over the same observation span. Three forms of the learning rate, α(t), as a function of the iteration number t are used:
linear: α(t)=α0(1−t/T); power: α(t) = α0(0.005/α0)t/T ; inverse: α(t) = α0/(1 + 100t/T ),
where α0 is the initial learning rate and T is the total number of iterations. In Figure 12.9, the functions α(t) are drawn for the linear, power, and inverse forms, where we have taken α0 = 0.5 and T = 100. Like α, σ in (12.6) is also taken to decrease monotonically.
12.5.3 Batch Version
The batch SOM algorithm is significantly faster than the on-line version. As before, we first make an initial choice of representatives {mk}. For the kth node, we list all those items xi whose mk∗ ∈ Nc(k). Then, we
￼
12.5 Self-Organizing Maps (SOMs) 435
￼￼￼0.5 0.4 0.3 0.2 0.1 0.0
Linear
Power
Inverse
020406080100 t
￼alpha(t)
￼FIGURE 12.9. Graphs of the on-line SOM learning-rate α(t) as a func- tion of the iteration number t for the linear, power, and inverse forms, where the initial learning rate α0 = 0.5 and the total number of iterations is T = 100.
update mk by averaging the items obtained from the previous step of the algorithm, where we might use a weighted average, with weights {hik∗ } given by (12.6). Finally, repeat the process a few times.
In a batch SOM display, the nodes are drawn as circles, and the data points that are mapped to a node are then randomly plotted within the cir- cle corresponding to that particular node; see Figure 12.10, which presents a SOM display of the Landsat data. This can be a very useful graphical dis- play for showing the interrelated structure of the (often high-dimensional) representatives in a 2D plot, together with the input points that are mapped to each representative.
If each data point has a unique identifier, such as a gene description, then it is not difficult to determine the identities of the data points that are captured by each node. In many clustering problems, however, individual points do not have unique identifiers; so, instead, class membership can be used as a plotting symbol in the SOM plot, as in Figure 12.10. From a SOM plot, cluster patterns should be visible.
12.5.4 Unified-Distance Matrix
A different type of visualization of the cluster structure of a SOM is a U-matrix, where U stands for “unified distance” (Ultsch and Siemon, 1990). Each entry in a U-matrix is the Euclidean distance (in input space) between neighboring representatives. For example, if we have a map with one row of five nodes with representatives {m1, m2, m3, m4, m5}, then the
436
12. Cluster Analysis
￼￼￼￼￼￼333333333333433
11 3 3 113133331131333
3133333 33333333333333333333333333333333333333333333333333333333333333333313333333333333333333333 33333333333333333333333333333333333333333333333333333333333333333 33333333331333 33
53 3 3153131115511111511333331
12151115 1 2515551121145174111512245515151
11111111111111 111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111 111111111111111111111111111111111111111111111111111111111111111111111111111111111111111 1
1111111 111 111111111151111111111111111111111111111111115111111 11111111111111111111111111111111111111111
1111 11 1 11111111111111111111111111111111111111111111111111111111111111111111111
7 772 2577147143114774747 4 1523744455777434
3333433333333333333333333333333333343333333333433343333333333333333333343333333334333333333333333334343333333433333143333333333333333333333333333333334333333333333333333 3 43333333333333333333333333333333333333333333333333333333333333333333333343433333333333333333333333333333333333333333333333333334333333333333 3 33333333333334333333333343333334 3
31311311311333131 3 3 333 33
11111111111111111111
111111111
1111111111111111111111111111111111111111 1111111 11 1
￼￼￼￼￼￼2
222222222222 22222222222222222222222222222222222222
2 1 52222 222222324222222222222222
1111111111 111111151111111111111111111111111111115111111511111111111111111111111111111111111111111111
1 11111111111111111511111111511111111111111111111111111111111111511111 1
75 7 7 777 777 7 7
3333333433333334337333333 3
443333334
3 3 3
7 134333343333333334 1
1511111111111111111111115 11111151111511111151111111115115111111111511111115115511511111111111111111151111111111111 111111111111111111511111151111111111111111111111111111111111111111111
11
2222222222222222222 22 22
1113333553113131115 3
3333333333333333343344337333333333334333333333333334334333333333343333433337333333333333333333333333333333333333334343333333374333333 3433334434343334334344333333333333433334344343333333333333333334333334333333333333433
522222224222
2222 55 31
31333333 41 3
4
￼￼￼￼￼￼22222222222222222222222222222 2 222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222 2
22222221222222 22222222222522222222222222222222222222222522222 22222222222222
1 11111115151511 1 111115114111111111115111111111211114111111111151111111511112151111111111111155111111115111111111115111111111111 111111111111115111112111111111511115111
373117333 717337333 3
33373333334343744 33433373434343443347744434334344337733333343334774337343344334334334333334443443744344373433337433333333333434 4 73334443433334434444433343377437373433434433433
2222222222222222222222222222222222222222222222222222222222222222222
5
33 331333373533333333334133
￼￼￼￼￼￼5
52522555255252455542222554255552552 27 25
1 415151145155115514171111511 1111152511112
73737743444477354747773771374374541 3 74734777574374734174
3 347
73 44734444444477474444444444444444444444774474747 444444444743
2
5 552555225525525425522725725255555215555 5
7777
447447 77434444474774477444447474447444474747474444474447444447747
44777 43774747747447443777744774744474774777744744743474747 77447747477474474747744473477 4
1115111111111111111111
555 7551 7 7 77477 4
43774747777774
1
￼￼￼￼￼￼5555555 5555555555555515555555555555 5555155555555555555555557555555555555555755555555555555255552555555555555555
575 5555577 5
77777777777777777777777777777 7 77777777457755777775777 5
74 7 77 77757 755777755575577577575475777577
7 7 7 7575477777555 47757777757757777755775777477774 7
4 477777 477474474447477747347774 7447447477347477757444743444473474475777744747774447474447444447744
5 555555555555555555555555555555555555555
5755577757 7 5
45744474577445577577457
55555555727775755554555555 5557572775755575517752 575
4774477434544777474434747544445577447747 557777
FIGURE 12.10. A 6×6 hexagonal batch-SOM plot of the Landsat satellite image data. The circles correspond to nodes, and the projected points are plotted randomly within the appropriate circle to which they were deemed closest. The six classes of vegetation are used as plotting symbols (1=red, 2=blue, 3=turquoise, 4=purple, 5=yellow, 7=black).
U -matrix is a (1 × 9)-vector,
U = (u1, u12, u2, u23, u3, u34, u4, u45, u5), (12.7)
where uij = ∥ mi − mj ∥ is the Euclidean distance between neighbor- ing representatives, and ui is a representative-specific value; for example, u3 = (u23 + u34)/2 is the average distance from that representative to all neighboring representatives. A small value in a U-matrix indicates that the SOM nodes are close together in input space, whereas a large value indi- cates that the SOM nodes, even though they are neighbors in output space, are quite far apart in input space. Thus, the U-matrix provides a useful guide to the underlying probability density function of X projected onto two dimensions.
Rather than displaying these U-matrix values as a 3D landscape (with low valleys showing clusters and high ridges showing separations between clusters), it is usual instead to discretize the distance values and then color- code them in a 2D colormap, where the colors show the gradations in values. In the SOM Toolbox for Matlab, for example, large distances in the U- matrix are colored as yellow and red and indicate a cluster border, whereas
￼￼￼￼￼￼5 5555555 5
5 7 777 7 77777777775777777757777777777 777577777777777777777777777777757777777777777757777777777777777777777777777777757777777777775777777777
7777775 7
7 77777477777777777777777777777777777777 77777777777777777777777777777777777777777777777777477777777777777777777777777777777777777777777777777777777777777777777
7 7 777474777777777777777
77 7 7577457777777477 7 7772777754477777745777777777777474774754775777774777777757774 777775777577477555477774755757777777
7 7 77 7744
5 55555555555555555555555555575555555555555 555555555555555
7777777777775777777755777777777777777557777777
7777777777777777777777777777477777777777777777777777 7
7777777777777777777777774777777777777477777777777777777777774777777777777777777 7 777777777777777777777777777777777
774777777547745777744777774745777474477775774777 7747777777777554
￼U−matrix
12.5 Self-Organizing Maps (SOMs) 437
FIGURE 12.11. The U-matrix from the batch SOM with hexagonal grids for the Landsat satellite image data.
small distances are colored as blue and indicate items in the same cluster. Figure 12.11 displays the U-matrix with an hexagonal grid for the Landsat image data, where a number of clusters are visible.
An hierarchical SOM (HSOM) is a tree of maps (U-matrices), where the “lower” maps on the tree act as a preprocessing stage to the “higher” maps. As we climb up the hierarchy, the information becomes more abstract. HSOMs have been successfully used in the development of bibliographic information retrieval tools. For example, a “document map” has been cre- ated for organizing astronomical text documents (Lesteven, Poin ̧cot, and Murtagh, 2001). Using more than 10,300 articles published in several lead- ing astronomy journals, the authors selected 269 keywords, each of which appeared in at least five different articles. By clicking on an individual node in the map, information about the articles located at that node can be retrieved. From this information, the user can then access article content (title, authors, abstract, and the on-line full paper).
12.5.5 Component Planes
An additional useful visualization tool is a colormap of the various com- ponent planes. In general, the “components” are the individual input vari- ables that make up each item x.
Figure 12.12 shows the 36 component planes for the Landsat data. Be- cause these data have an easily visualized physical structure, the compo- nent planes are arranged into four groups of nine images (corresponding to the four spectral bands and the nine positions). The component planes
4.47
2.34
0.219
￼438
12. Cluster Analysis
TL1
CL1
BL1
TL3
CL3
BL3
TC1 TR1 TL2 TC2 TR2
92.3 92.6 92.3 115 115 113
68.7 68.9 68.8 74 73.9 73.2
45 45.2 45.4 32.8 32.7 33.2 dddddd
CC1 CR1 CL2 CC2 CR2
93 93.4 92.9 116 115 114
68.9 69.1 68.9 74 73.7 73.2
44.9 44.8 45 32.4 32 32.2 dddddd
BC1 BR1 BL2 BC2 BR2
92.9 92.8 91.9 115 115 113
69.1 68.8 68.4 73.7 73.5 72.4
45.3 44.9 44.9 32.2 32.4 32.1 dddddd
TC3 TR3 TL4 TC4 TR4
128 127 126 136 136 135
98.8 99 99.3 95.3 95.9 95.5
70 70.7 72.2 54.3 55.6 56.4 dddddd
CC3 CR3 CL4 CC4 CR4
128 129 128 137 138 136
99.1 99.5 99.4 95.7 96.4 95.8
70.5 70.4 71.1 54.4 55.1 55.5 dddddd
BC3 BR3 BL4 BC4 BR4
126 128 127 134 137 136
98.9 99.5 99.5 95.2 96.2 96.1
72 71.3 71.8 56.2 55.6 56.1 dddddd
FIGURE 12.12. Colormaps of the 36 component planes from the batch- SOM algorithm with hexagonal grids for the Landsat image data. The com- ponent planes are arranged into four groups (corresponding to the four spec- tral bands, 1, 2, 3, and 4), each group having nine component planes (cor- responding to the nine positions (TL, TC, TR; CL, CC, CR; BL, BC, BR, where T is top position, C is center, B is bottom, L is left, C is center, R is right) in the 3×3 pixel neighborhoods.
show that the variable values differ substantially between the four spectral bands. Within each set of 3×3 pixel neighborhoods, the component planes show some differences, but those differences are not as significant as be- tween spectral bands. In this example, the component planes have given us a good view of the differences in measurement of each of the four spectral bands.
The U-matrix and component planes derived from SOMs have been ap- plied to the visualization of gene clusters derived from microarray data (see, e.g., Tomayo, Slonim, Mesirov, Zhu, Kitareewan, Dmitrovsky, Lander, and Golub, 1999). In particular, if the genes are expressed at different points in time or at different temperatures, then the component planes, which can be thought of as “slices” of the U-matrix, show the cluster structure obtained at each timepoint or temperature.
12.6 Clustering Variables
We can use the same clustering methods for variables as we used for clustering observations, the main difference being the measure of distance between variables. For clustering variables, we generally use a distance met- ric based upon the correlation matrix for the r variables. The correlations provide a reasonable measure of “closeness” between pairs of variables. Those pairs of variables with relatively large correlations can be thought of as being “close” to each other; those pairs for which the corresponding correlations are small are considered to be “far away” from each other.
If we standardize each of the r variables across the n observations to have zero mean and unit variance, then it is not difficult to show that
1 􏰏n
12.6 Clustering Variables 439
￼(xji −xki)2 = 1−ρjk ∈ [0,2], (12.8) where ρjk is the sample correlation between variables Xj and Xk. This
￼2(n−1)
shows us that using squared Euclidean distance, 􏰊 (xji−xki)2, is equivalent
i=1
i
to using 1−ρjk as a dissimilarity measure. Either distance metric enables us
to utilize any of the hierarchical or nonhierarchical/partitioning clustering methods discussed above, and the graphical output can be a dendrogram or a silhouette plot as appropriate.
12.6.1 Gene Clustering
The most popular use of variable clustering has been in clustering the thousands or tens of thousands of genes measured using a microarray ex- periment. Concern over the enormous volume of biological information in an organism’s genome has led to the idea of grouping together those genes
440 12. Cluster Analysis
with similar expression patterns. This type of clustering is referred to as gene clustering, where, in addition to the usual hierarchical and partition- ing methods, some specialized methods have been developed.
In gene clustering, the (r × n) data matrix X = (xij ) contains the gene- expression data derived from a microarray experiment, where i indexes the row (gene), j indexes the column (tissue sample), and xij is, for example, the intensity log-ratio of the abundance of the ith gene in the experimental sample relative to some reference sample; in other words, xij is a measure- ment of how strongly the ith gene is expressed in the jth sample. Because xij is the log of a ratio, it follows that those ratios with values between 0 and 1 will yield negative xij , whereas those ratios greater than 1 will yield positive xij . For typical microarray experiments, r ≫ n, so that the matrix X will be “vertically long and skinny.”
12.6.2 Principal-Component Gene Shaving
Suppose our goal is to discover a gene cluster that has high variability across samples. Let Sk denote the set of (row) indices of a cluster of k genes. Consider the jth tissue sample (i.e., jth column of X) and compute the average gene-expression over the k genes for that sample,
x ̄j,Sk = 1 􏰏 xij, j = 1,2,...,n. k i∈Sk
Compute the average of (12.9) over all n tissue samples,
(12.9)
(12.10)
( 1 2 . 1 1 )
￼x ̄Sk = n
The empirical variance of x ̄Sk is defined by
1 􏰏n
1 􏰏n 􏰏 x ̄j,Sk = kn xij.
￼￼j=1
v􏰑a r { x ̄ S k } = n
j=1 i∈Sk
( x ̄ j , S k − x ̄ S k ) 2 .
1 􏰏n
￼j=1
Given all possible clusters of size k, we can search for that cluster Sk with
the highest v􏰑ar{x ̄Sk }. Unfortunately, such a search procedure is computa-
tionally infeasible because it entails evaluating 􏰉r􏰀 different subsets, which k
gets big very quickly for r large, as would be common in gene clustering.
Gene shaving (Hastie, Tibshirani, Eisen, Alzadeh, Levy, Staudt, Chan, Botstein, and Brown, 2000) has been proposed as a method for clustering genes, where the primary goal is to identify small subsets (i.e., clusters) of highly correlated (“coherent”) genes that vary as much as possible between
samples. This method differs from those described previously in that genes are allowed to be included as members of more than one cluster.
Consider a linear combination of the jth column gene expressions,
􏰏r
zj = aτxj =
where xj = (x1j,···,xrj)τ, a = (a1,···,ar)τ, the {ai} are positive, neg-
ative, or zero weights, and 􏰊r a2 = 1. For example, for given k, we
√
coefficients {ai} such that the variance of Zj is maximized.
i=1 i
could set ai = ±1/ k for i ∈ Sk, and zero otherwise. We wish to find the
i=1
12.6 Clustering Variables 441
aixij, (12.12)
￼The solution is given by the first principal component (PC1) of the r rows of X . The min(r − 1, n) principal components of X are referred to as eigen- genes. The individual genes may be ordered according to the magnitude (from largest to smallest in absolute value) of their respective coefficients in the first eigen-gene PC1; we expect that many of the coefficients in PC1 will be close to zero. We could threshold those “near-zero” coefficients (i.e., set the coefficient value equal to zero if it is smaller than a prespecified limit), thereby removing those particular genes from the cluster, but, from experience with simulations, we can do better.
As a selection process for weeding out unimportant genes, we instead compute the inner product (or correlation) of each gene with PC1 and “shave off” (i.e., remove) those genes (rows of X) with the 100α% smallest absolute inner products (e.g., α = 0.1). This shaving process decreases the size of the set of available genes, say to k1 genes. From the reduced subset of k1 rows, we recompute the first principal component, which, in turn, is shaved to a subset of, say, k2 rows. This iteration is repeated until a finite sequence of nested gene clusters, Sr ⊃ Sk1 ⊃ Sk2 ⊃ ··· ⊃ S1, is obtained, where Sk denotes the set of indices of a cluster of k genes.
The next step is to decide on k and Sk. For a given value of k, define the following ANOVA-type decomposition of the total variance,
where
1 􏰏 􏰏n
VT = kn
1 􏰏n
VB = n (x ̄j,Sk −x ̄Sk)2,
(xij −x ̄Sk)2 = VB +VW,
(12.13)
(12.14) ( 1 2 . 1 5 )
￼i∈Sk j=1
￼j=1 􏰬
V W = 1 􏰏n n j=1
1 􏰏 ( x i j − x ̄ j , S k ) 2 k i∈Sk
􏰭
￼￼
442 12. Cluster Analysis
are the between-variance and within-variance, respectively. A natural statis- tic is
R2(Sk) = VB × 100% = VB/VW × 100%, (12.16) VT 1 + VB /VW
which is the percentage of the total variance explained by the gene cluster Sk. The larger the value of R2, the more coherent the gene cluster.
Hastie et al. now determine the cluster size k by a permutation argument
applied to the R2-value in (12.16). The “significance” of the R2-value is
judged by comparing it with its expectation computed under a suitable ref-
erence null distribution; in this case, the reference distribution assumes the
rows and columns of X are independent. Randomly permute the elements
of each row of X to get X∗. Do this B times to get X∗b, b = 1,2,...,B.
Apply the shaving algorithm to X∗b, that gives S∗b, and then compute
R2(S∗b), b = 1,2,...,B. k
￼￼The gap statistic (Tibshirani, Walther, and Hastie, 2001) is defined as
Gap(k) = R2(Sk) − R2(Sk∗), (12.17)
w h e r e R 2 ( S ∗ ) i s t h e a v e r a g e o f a l l t h e { R 2 ( S ∗ b ) , b = 1 , 2 , . . . , B } . We c h o o s e kk
that value, 􏰡k, of k (and, hence, S􏰡k) which results in the maximum gap; that is, 􏰡k = arg maxk Gap(k). A useful graphical technique is to plot the gap curve, which is a plot of Gap(k) against cluster size k. Set 􏰡k = 􏰡k(1).
After determining the number, 􏰡k(1), of genes and their identities, we
look for a second gene cluster. Before we do that, we need to remove the
effects of the first cluster of genes. Hastie et al. apply an orthogonaliza-
k
￼￼(1) (1) (1) τ tion trick: first, compute the first supergene, x ̄ = (x ̄1 , · · · , x ̄n )
, a row vector of average genes corresponding to the first cluster S􏰡k(1) , where
x ̄(1) = 􏰊 xij /􏰡k(1), j = 1, 2, . . . , r; second, orthogonalize X by re- j i∈S􏰡k(1)
gressing each row of X on the supergene x ̄(1) and replacing the rows of X by the residuals from each such regression. This gives us the matrix X1. Rerun the shaving algorithm on X1 and then use the gap statistic to ob- tain 􏰡k(2), the second gene cluster S􏰡k(2) , and the second supergene x ̄(2) This process is applied repeatedly a total of t times, where t is prespecified, by modifying X and x ̄ at each step; at the kth step, X is orthogonal to all the previously obtained supergenes x ̄(l), l = 1, 2, . . . , k − 1.
One of the main steps in the gene-shaving process is the use of the gap statistic to determine the cluster size k. Hastie et al. report good results for the gap statistic when the clusters are well-separated. However, there is evidence that the gap statistic tends to overestimate the number of clusters (Dudoit and Fridlyand, 2002; Simon et al., 2003, p. 151).
After identifying each gene cluster, the rows of X can be reordered to display those gene clusters more explicitly. The tissue samples (columns of
X) can also be reordered according to either the average gene expression of each column of X or some external covariate reflecting additional infor- mation, such as tissue type or cancer class. A supervised version of gene shaving (Hastie et al., 2000) has been developed, which, for example, is able to identify gene clusters that are closely associated with patient survival times.
12.6.3 Example: Colon Cancer Data
We apply PC gene-shaving to the colon cancer microarray data described in Section 2.2.1. The microarray data consist of expression levels of 92 genes obtained from a microarray study on 62 colon tissue samples. The gene- expression heatmap for the colon cancer data is displayed in Figure 2.1.
Figure 12.13 shows the gap curves for the first four clusters derived using the gene-shaving algorithm. For each cluster, the value of k at which the gap curve attains its maximum is chosen to be the estimated size of the cluster. The estimated cluster sizes for the first four clusters are 41, 15, 6, and 19, respectively. The four heatmaps for those gene clusters are displayed in Figure 12.14, where the samples are ordered by the values of the column averages; each panel gives the values of the total variance VT , the between- variance VB , the ratio VB /VW , and R2 = VB /VT × 100%, the percentage of the total variance explained by that cluster. The largest R2 value was that of the third cluster at 64.8%.
The four clusters in Figure 12.14 display different patterns of gene ex- pression. The first cluster has an interesting feature in that the genes split into two equal-sized subgroups: for a given tissue sample, when the “up- per” subgroup of genes are strongly upregulated (red color), the “lower” subgroup are strongly downregulated (green color), and vice versa. Fur- thermore, the red/green split depends upon whether the sample is a tumor sample or a normal sample. The second and third clusters of genes have the same overall appearance: in both, the tumor samples (mostly located on the right of the heatmap) tend to be upregulated, whereas normal sam- ples (mostly located on the left of the heatmap) tend to be downregulated. The reds and greens of the fourth cluster are somewhat more randomly sprinkled around the heatmap, although there are pockets of adjacent cells (e.g., the top few rows and a portion of the right-hand side) that seem to share similar expression patterns.
12.7 Block Clustering
So far, our focus has been on clustering observations (cases, samples) or variables separately. Now, we consider the problem of clustering observa- tions and variables simultaneously.
12.7 Block Clustering 443
￼
444 12. Cluster Analysis
GeneShaveGapCurveGraphs
Cluster#1 Cluster#2
510 50100 510 50100 ClusterSize ClusterSize
Cluster#3 Cluster#4
510 50100 510 50100 ClusterSize ClusterSize
FIGURE 12.13. Gap curves for the first four clusters of colon cancer data. The gap estimate of cluster size is that value of k for which the gap curve is a maximum. The estimated cluster sizes are first cluster (top-left panel), 41; second cluster (top-right panel), 15; third cluster (bottom-left panel), 6; and fourth cluster (bottom-right panel), 19.
The simplest way to do this is to apply a hierarchical clustering method to rows and columns separately. Figure 12.15 displays the heatmap of the colon cancer data, where rows and columns have been rearranged through separate hierarchical clustering algorithms. We see a partition of the heatmap into blocks of mainly reds or greens. The rearrangement of rows (colon tissue samples) does not correspond to the known division into tumor samples and normal samples.
Block clustering, also known as direct clustering (Hartigan, 1972), pro- duces a simultaneous reordering of the rows and columns of the (r × n) data matrix X = (xij ) so that the data matrix is partitioned into K sub- matrices or “data clusters.” As an example, Hartigan (1974) clustered the voting records of 126 nations on 50 selected issues at the United Nations, where each vote was coded as 1 (= yes), 2 (= abstain), 3 (= no), 5 (= absent), or 0 (= unknown), and the “absents” are treated as missing data. To motivate the two-way clustering, a natural problem was whether “blocs” of countries exist that vote alike on “blocs” of questions that arise from the same issue.
￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼GapCurve GapCurve
10 15 20 25 30 253035404550
GapCurve GapCurve 1015202530 1520253035
￼J02854 T60155 T92451 R87126 M63391 M64110 Z50753 H06524 T67077 X12496 U32519 T51023 T96873 H08393 H40095 T61609 X14958 T51529 T86749 M26697 M22382
X70944 T57633 T57630 H20819 X15183 X70326 U26312 R75843 D31885 X13482 X12671 X53586 X74262 U29092 L41559
L25941
X12466
R84411
T83368
H55916
R08183
X55715 X63629 T47377 X86693 M63391 T92451 M64110 T40454 T86749 X74295
Z49269_2 R52081 T51023 T96873 L08069 Z49269 X62048 U19969 H87135
GG ee nn ee SS hh aa vv ee CC lluu ss ttee rr PP lloo ttss Cluster#1
eigenvalue=1529.4749%variance=60.8779VT=0.9839VB=0.599VB/VW=1.5561 Cluster#2
eigenvalue=378.8942%variance=55.9528VT=0.7223VB=0.4041VB/VW=1.2703
GG ee nn ee SS hh aa vv ee CC lluu ss ttee rr PP lloo ttss Cluster#3
eigenvalue=168.2754%variance=64.8039VT=0.6946VB=0.4501VB/VW=1.8412 Cluster#4
eigenvalue=267.089%variance=46.9138VT=0.4714VB=0.2212VB/VW=0.8837
Normal12 Normal10 Normal19 T11 T2 Normal4 Normal3 T2 T17 Normal3 Normal5 T25 T11 Normal5 Normal10 T17 Normal2 Normal14 Normal13 T4 Normal11 Normal22 Normal4 Normal9
Normal8 T33 T36 Normal11 T25 T6 Normal7 Normal16 T18 T36 Normal8 T18
T4 Normal9 Normal12 T14 Normal9 Normal12 T3 Normal12 T1 Normal13 Normal16 Normal2 Normal4 Normal17 Normal14 T15 T19 T2 T37 T30
T13 T20 T2 T1 T16 Normal11 Normal11 T32 T14 Normal15 T8 T13
Normal18 T30 T7 Normal21 Normal21 Normal20 T19 T19 Normal1 Normal7 Normal17 T6 T8 T21 T12 T10 T15 T8 T25 T16 Normal7 T19 T30 Normal20 T30 Normal6 Normal9 T33 T38 T34 Normal21 Normal4 T23 Normal18 Normal2 Normal18 T9 Normal21 Normal22 T28 T7 Normal16 T13 Normal5 T12 T29 T28 Normal1 T28 T12 T32 Normal14 Normal6 T9 T35 T23 T35 T4 T33 T40 Normal5 T37 T22 Normal8 T10 T13 Normal1 Normal10 Normal13 T3 T10 T38 Normal16 T31 T26 T22 T5 T32 Normal15 T12 Normal19 Normal2 Normal20 Normal6 T33 T18 Normal6 T37 T22 Normal1 Normal18 T35 Normal20 Normal8 T5 T8 T6 T15 T16 T31 T40 T1 T17 T5 Normal10 Normal19 T9 T7 T21 T10 T34 T9 T32 T7 T4 Normal15 T37 T40 T39 T21 T39 T35 T1 Normal3 Normal14 T11 T24 T34 T27 T26 T15 T24 Normal3 T25 T11 T29 T3 T22 T38 Normal7 T26 T27 T23 T20 T34 T39 T20 T3 T24 T24 T14 Normal19 Normal22 T17 T21 T39 T29 T23 T31 T27 Normal15 T28 T40 Normal13 T31 T5 T18 Normal22 T20 T38 T27 T26 Normal17 T16 T6 T36 T36 T14 T29 Normal17
-3-10123 -4-202 -4-2012 -3-10123
FIGURE 12.14. Heatmaps for the first four gene clusters for the colon cancer data, where each cluster size is determined by the maximum of that gap curve. The genes are the rows and the samples are the columns. The samples are ordered by the values of the column averages.
12.7 Block Clustering 445
￼446 12. Cluster Analysis
54 19 52
FIGURE 12.15. Separate hierarchical clustering of rows (colon tissue samples) and columns (genes) of the colon cancer data.
In block clustering, each entry in the data matrix appears in one and only one data cluster, and each data cluster corresponds to a particular “row cluster” and a particular “column cluster.” The block-clustering algorithm given in Table 12.8 partitions the rows and columns of X into homogeneous, disjoint blocks (i.e., where the elements of each block can be closely approx- imated by the same value) so that the row clusters and column clusters are hierarchically arranged to form row and column dendrograms, respectively.
12.8 Two-Way Clustering of Microarray Data
For clustering gene expression data, it can be argued that creating dis- joint blocks of genes and samples may be an over-simplification of the sit- uation. Biological systems are notoriously complicated, and interrelations between these systems may result from some genes possessing multiple
24 32 912 21 14 15 22 622 33 31 511 32 31 11 12
37846
33 55 45 44 54 65 56 54 244 33
70 90
62 34 8
18 91
64 14 23
02 58 8
57 95
80 76 94 73 43 05 27 51 62
61
30
MM
MMD
77T
MUT
RTT
HUH 004 890 350 969 MRJ 230 665 690 973 MTT 258 216 307 824 RXL 506 282 000 864 MUX 321 264 539 185 HXT 985 674 819 734 XXT 451 055 471 518 RTT 467 749 311 715 XXT 565 236 165 829 UDH 164 730 885 976 UHX 221 603 384 118 DXT 807 600 453 792 XTT 555 773 665 338 DXX 317 120 869 874 RXL 774 451 285 645 HUT 552 519 950 179
Z
MHT 664 403 178 178 087 MHX 308 666 656 329 443 UXZ 527 054 712 539 385 HRT 778 717 501 922 756 MUZ 461 939 239 696 919 4MH 991211647961.392
DXT TTJ HRL
641 722 004 749 776 690 022 148 555 514 670 485 491 834 944 782 609 388 710 858 66 6 33 2 77 9 88 4 .. 732 393 650 908 812 185
456 211 556 020
199 345 772 239 198 938 352 453 752 597 940 292 366 306 514 239 612 131 86
RRL 802 485 419 184 XT 81 32 34 66
12.8 Two-Way Clustering of Microarray Data 447
TABLE 12.8. Hartigan’s block-clustering algorithm.
1. Start with all data in a single block (i.e., K = 1).
2. Let B1, B2, . . . , BK denote a partition of the rows and columns of X into K blocks (or data clusters), where Bk = (Rk,Ck) consists of a set, Rk, of rk rows and a set, Ck, of ck columns of X, k = 1,2,...,K.
3. Within the kth block Bk, compute x ̄k, the average of all the xij within
￼that block. Approximate X by the matrix X􏰡 = (x􏰡ij), where the x􏰡ij = x ̄k
are constant within block Bk. Compute ESS = 􏰊K 􏰊 (xij −x ̄k)2,
k=1 (i,j)∈Bk
4. At the hth step, there will be h blocks, B1,B2,...,Bk,...,Bh. Suppose we
the total within-block variance.
destroy Bk by splitting it into two subblocks, B′ and B′′, either by splitting
kk
the rows or the columns. Consider a row-split of the block Bk = (Rk,Ck).
Suppose Rk contains a previous row-split of a different block Bl = (Rl, Cl) into B′ = (R′ , C′ ) and B′′ = (R′′, C′′). Then, the only row-split allowable
llllll
for Bk is a fixed split given by R′ = R′ and R′′ = R′′. Similarly for column klkl
splits. A free split is a split in which no such restrictions are specified.
5. The reduction in ESS due to row-splitting Bk into B′ and B′′ is given by
ΔESS = ckr′ [x ̄(B′ ) − x ̄(Bk)]2 + ckr′′[x ̄(B′′) − x ̄(Bk)]2, kk kk
where x ̄(B) denotes the average of X over the block B.
6. At each step, compute ΔESS for each (row or column) split of all existing
blocks. Choose that split that maximizes ΔESS.
7. Stop when any further splitting leads to ΔESS becoming too small or when
the number of blocks K becomes too large.
functions. Hence, it may be more realistic to accept the idea that certain clusters should naturally overlap each other. Furthermore, similarities be- tween related genes and between related samples may be more complex due to gene-sample interaction effects.
12.8.1 Biclustering
With this in mind, the biclustering approach (Cheng and Church, 2000) seeks to divide the (r × n)-matrix X = (xij ) of gene-expression data into a pre-specified number of “biclusters,” which do not have to be disjoint. Each bicluster corresponds to a subset of the genes and a subset of the samples that possess a high degree of similarity. So, certain rows and columns of X will appear in several biclusters. The basic idea is to determine in a sequential fashion one bicluster at a time.
kk
￼
448 12. Cluster Analysis
A bicluster is defined as a submatrix, X (I, J ), of X , where I is a subset of nI rows and J is a subset of nJ columns in X . Consider the expression level xij , i ∈ I, j ∈ J . If we model the bicluster by an additive two-way analysis of variance (ANOVA) model, then we can write
xij≈μ+αi+βj, i∈I,j∈J, (12.18) where μ is the overall mean effect, αi represents the effect of the ith row, βj
the effect of the jth column, and, for uniqueness, we assume that 􏰊 αi = 􏰊 i∈I
Let
j∈J βj = 0. Least-squares estimates of μ, αi, and βj are given by
where
μ􏰡=x ̄, α􏰡=x ̄ −x ̄, β􏰡=x ̄ −x ̄, ·· i i· ·· j ·j ··
x ̄i·=n−1􏰏xij, x ̄·j=n−1􏰏xij JI
j∈J i∈I x ̄·· = (nInJ )−1 􏰏 􏰏 xij.
(12.19) (12.20) (12.21)
(12.22)
i∈I j∈J The least-squares residual at xij is defined as
􏰏􏰏
H(I,J) = RSS(I,J), (12.24) nI nJ
which is proportional to the residual mean square RMS(I,J) for the bi- cluster; that is, RMS = [nInJ /(nI − 1)(nJ − 1)]H. The aim is to find a row set I and a column set J such that H(I,J) has a small value.
A bicluster is constructed by sequentially deleting one or multiple rows or columns at a time from X , where the choice is determined at each step so as to achieve the largest decrease in the value of H. Deleting rows or columns will reduce the value of H. A similar result allows one to add some rows or columns without increasing H. Like all greedy algorithms, this algorithm needs a threshold value; it is usual to fix a maximum-acceptable threshold δ ≥ 0 for the value of H while running the algorithm.
As each bicluster is found, the elements of X corresponding to that bi- cluster are replaced by random numbers (so that no recognizable pattern from that bicluster is retained that could be correlated with future biclus- ters), and the next bicluster is sought. The random numbers are sampled from a uniform density over a range appropriate for the given application.
e􏰡 =x −μ􏰡−α􏰡 −β􏰡 =x −x ̄ −x ̄ +x ̄ , i∈I,j∈J.
ij ij
i j ij i· ·j ··
R S S ( I , J ) =
e􏰡 i j
(12.23) be the residual sum of squares for the bicluster. The objective function is
i∈I j∈J
2
￼
12.8 Two-Way Clustering of Microarray Data 449
12.8.2 Plaid Models
Plaid models (Lazzeroni and Owen, 2002) form a family of models for car- rying out two-way clustering, in which sums of “layers” of two-way ANOVA models are fitted to gene-expression data. As such, it generalizes the bi- clustering approach. Each “layer” is formed by a subset of the rows and columns and can be viewed as a two-way clustering of the elements of the data matrix, except that genes can be members of different layers or of none of them. Hence, overlapping clusters (i.e., layers) are allowed.
There are several different types of plaid models, some more detailed than others. Consider the following simple model,
􏰏K k=1
In this model, μ0 represents the expression level for the background layer, μk represents the expression level in the kth layer, and ρik and κjk are two indicator variables, each of whose value is either 1 or 0 depending upon i, j, and k. Thus, ρik = 1 (or 0) indicates the presence (or absence) of the ith gene in the kth gene-layer, whereas κjk = 1 (or 0) indicates the presence (or absence) of the jth sample in the kth sample-layer. The expression level μk is said to be upregulated if μk > 0 and downregulated if μk < 0.
Requiring each gene and each sample to be in exactly one cluster would mean that 􏰊 ρik = 1 for every i, and 􏰊 κjk = 1 for every j, respectively.
xij ≈ μ0 +
μkρikκjk. (12.25)
kk
To allow overlapping levels, these constraints would have to be relaxed: for example,wecouldset􏰊 ρik ≥2forsomei,or􏰊 κjk ≥2forsomej.We
kk
would also need to recognize that there may be genes or samples that do not belong naturally to any layer; for such genes, 􏰊 ρik = 0, and for such
􏰊k
samples, k κjk = 0. In general, we do not need to impose any restrictions
on the {ρik} and {κjk}.
A more general ANOVA-type model is given by
􏰏K
xij ≈ μ0 + (μk + αik + βjk)ρikκjk, (12.26)
k=1
where αik and βjk measure the effects of the ith row (genes) and jth column
(samples), respectively, in the kth layer. To avoid overparameterization, we
require 􏰊 ρikαik = 􏰊 κjkβjk = 0, k = 1,2,...,K. The description of ij
model (12.26) as a “plaid” model derives from the visual appearance of the fitted heatmap of μk +αik +βjk, where we see the row-stripes of the {ρik} and the column-stripes of the {κjk}.
Letθijk =μk+αik+βjk,k=1,2,...,K.Then,wecanwritetheplaid model (12.26) as
􏰏K k=1
xij ≈ θij0 +
θijkρikκjk. (12.27)
450 12. Cluster Analysis
To estimate the parameters {θijk} in (12.27), we minimize the criterion,
1􏰏r􏰏n􏰈 􏰏K 􏰙2
Q = 2 xij − θij0 − θijkρikκjk , (12.28)
i=1 j=1 k=1
with respect to {θijk}, {ρik}, {κjk}, where ρik, κjk ∈ {0, 1}. Given the num- ber of layers K, this optimization problem quickly becomes computation- ally infeasible (each gene and each sample can be in or out of each layer, and so there are (2r − 1)(2n − 1) possible combinations of genes and samples).
To overcome this problem, the minimization of Q is turned into an iter- ative process, where we add one layer at a time. Suppose we have already fitted K − 1 layers, and we need to identify the Kth layer by minimizing Q. If we let
￼K−1 􏰏
zij =xij −θij0 −
denote the “residual” remaining after the first K − 1 layers, then
(12.29) we can
(12.30) (12.31)
(12.32)
write Q as
k=1
(zij −θijKρiKκjK)2
(zij − (μK + αiK + βjK)ρiKκjK)2 .
1 􏰏r 􏰏n
Q = 2
1 􏰏r 􏰏n
θijkρikκjk
￼i=1 j=1
= 2
We wish to minimize Q subject to the identifying conditions
￼i=1 j=1
􏰏r i=1
αiKρ2iK =
􏰏n j=1
βjKκ2jK = 0.
From (12.31) and (12.32), we set up the usual Lagrangian multipliers, dif- ferentiate wrt μK, αiK, and βjK, set the derivatives equal to zero, and solve. The results give:
􏰊􏰊
∗ i j zijρiKκjK μK = (􏰊 ρ2 )(􏰊 κ2 )
∗
βjK =
Given the values of ρ(s−1) and κ(s−1) from the (s − 1)st iteration, we use
￼􏰊iiK jjK
j(zij −μKρiKκjK)κjK
(12.33) (12.34) (12.35)
ρiK(􏰊 κ2 ) 􏰊 jjK
αiK = ∗
(12.33)–(12.35) to update θ(s) at the sth iteration. Note that updating ijK
￼i(zij −μKρiKκjK)ρiK κjK(􏰊ρ2 ) .
￼iK jK
i iK
12.8 Two-Way Clustering of Microarray Data 451
α∗iK only requires data for the ith gene, and updating βj∗K only requires data for the jth sample; hence, the resulting iterations are very fast.
Given values for θijK , the update formulas for ρiK and κjK are found by differentiating (12.31) wrt ρiK and κjK, setting the results equal to zero, and solving. This gives:
(12.36)
􏰊 zijθijKκjK ∗j
ρiK = ∗
􏰊 θ2 κ2 􏰊j ijK jK
￼i zijθijKρiK 􏰊 θ2 ρ2 .
(12.37) So, set the initial values of all the ρs and the κs to be in (0, 1) (say, make
ijK jK
to update ρ(s). Similarly, given values of θ(s) and ρ(s−1), we use (12.37) to iK ijK iK
update κ(s) . The trick is to keep ρ and κ away from 0 and 1 early in the jK
iteration process, but to force ρ and κ toward 0 and 1 late in the process. At convergence, the estimated parameters for the kth layer are denoted by μ􏰡 k , α􏰡 i k , a n d β􏰡 j k , k = 1 , 2 , . . . , K .
The absolute values of the row effects, |μ􏰡k +α􏰡ik|, and the column effects, |μ􏰡 +β􏰡 |, for the kth layer (k = 1,2,...,K) can each be ordered to show
κjK =
them all equal to 0.5). Then, given values of θ(s) and κ(s−1), we use (12.36)
￼k jk
which genes and samples are most affected by the biological conditions of that layer. Within the kth layer, genes are upregulated if μ􏰡k + α􏰡ik > 0, whereas genes with μ􏰡k + α􏰡ik < 0 are said to be downregulated. The “size” or “importance” of the kth layer is indicated by the value of
􏰏n 􏰏r
σ2 = k
ρ∗ κ∗ θ2 , (12.38) ij jk ijk
i=1 j=1
and this quantity is used in a permulation argument by Lazzeroni and
Owen to choose the number of layers K.
12.8.3 Example: Leukemia (ALL/AML) Data
The data for this example4 are obtained from a study of two types of acute leukemias — acute lymphoblastic leukemia (ALL) and acute myeloid leukemia (AML) (Golub et al, 1999). The leukemia data, which consist of gene expression levels for 7,219 probes from 6,817 human genes, were
4The leukemia data can be found in the file ALL AML Merge.txt on the book’s website. The data are available in the Bioconductor R package golubEsets, and the preprocess- ing code is in the Bioconductor R package multtest, both of which can be downloaded from the website http://www.bioconductor.org.
i ijK iK
￼￼￼
452 12. Cluster Analysis
derived using Affymetrix high-density oligonucleotide arrays. There are 72 mRNA samples made up of 47 ALL samples (38 B-cell and 9 T-cell) and 25 AML samples extracted from bone marrow (BM) or from peripheral blood (PB).
The leukemia data were preprocessed following the methods of Golub et al. (see Dudoit, Fridlyand, and Speed, 2002): (1) a floor and ceiling of 100 and 16,000, respectively, were set for the expression levels; (2) any gene that has low variability (i.e., any gene with either max / min ≤ 5 or max − min ≤ 500) over all tissue samples was excluded; (3) the remaining expression levels were transformed using a logarithmic (base-10) transformation; (4) the preprocessed leukemia data were standardized by centering (mean 0) and scaling (variance 1) each of the mRNA samples across rows (genes). This left a data array, X = (xgi), consisting of 3,571 rows (genes) by 72 columns (mRNA samples), where xgi denotes the expression level for the gth gene in the ith mRNA sample.
We applied the plaid model to the leukemia data. Our strategy consisted of (1) four shuffles in the stopping rule; (2) a common sign for μ + αi and for μ + βj within each layer; and (3) any row (or column) is released from a layer if being part of a layer failed to reduce its sum of squares by at least 0.51. The algorithm stopped after finding 11 layers, each containing αi and βj components. After the 11th layer, the algorithm failed to find a layer that retained any rows under the release criterion.
Table 12.9 shows the composition of each of the 11 layers. We see that layer 4 is completely composed of AML samples, layer 5 consists of only ALL B-cell samples, and layers 3 and 11 contain only ALL samples. All other layers are mixed ALL and AML samples. Only 55 of the 72 samples are contained in the 11 layers, so that 17 samples were not included in any layer. The biggest percentage omission is for the ALL T-cell samples with 5 out of 9 samples not included; 9 of the 38 ALL B-cell samples and 3 of the 25 AML samples are omitted.
Table 12.10 gives the estimated column effects, μ􏰡k + β􏰡jk, in the first 8 layers; notice that the signs of each column effect are the same within each layer. We see a pattern of similar mRNA samples appearing in the odd layers 1, 3, 5, 7, and 11, and in the even layers 2, 4, 6, and 8. These odd-even patterns, however, are switched in layers 9 and 10.
While we see from Table 12.9 that the number of samples in the different layers is about the same, the number of genes decreases from more than 200 in the first few layers to a much smaller number in each of the last few layers. About half of the genes in each of the first two layers are the same, whereas a third of the genes in layer 3 are present in layer 4 and vice versa. The amount of gene overlap in the other layers is negligible.
12.9 Clustering Based Upon Mixture Models 453
TABLE 12.9. Plaid analysis of the leukemia data. Composition of each layer by the number of genes (rows) and number of samples (columns), and the number of ALL B-cells, ALL T-cells, and AML samples in each layer.
￼Layer Genes 1 230 2 222 3 265 4 238 5 61 6 13 7 15 8 3 9 11 10 5 11 10
Samples 14 16 13 19 14 16 13 17 17 14 10
ALL-B ALL-T AML 12 0 2 9 1 6 12 1 0 0 0 19 14 0 0 3 2 11 11 0 2 6 2 9 5 1 11 13 0 1 9 1 0
￼￼12.9 Clustering Based
Upon Mixture Models
￼So far, our treatment of clustering has been algorithmic; instead of cre- ating clustering methods based upon a statistical model with stochastic elements (so that the the full force of the traditional statistical inference framework could be applied), we have used nonstochastic methods whose computational solution in each case is an iterative algorithm. In this sec- tion, we adopt a mixture model approach to clustering. The EM algorithm, which is a general optimization routine for the treatment of incomplete data, has been found to be especially valuable for fitting mixture models, particularly in problems from machine learning, computer vision, vector quantization, image restoration, and market segmentation.
Suppose D = {xi,i = 1,2,...,n} denotes the complete set of data, assuming no missing values. The complete-data likelihood is given by
L(ψ|D) = p(D|ψ), (12.39) where p(D|ψ) is the joint density of D, and ψ is an unknown parameter
vector. Now, suppose some components of D are missing. We can write
D = {Dobs, Dmis}, (12.40)
where Dobs is the observed part of D, and Dmis is the missing part of D. If the probability that a particular variable is unobserved depends only upon Dobs and not on Dmis, then the observed-data likelihood is obtained by integrating Dmis out of the complete-data likelihood,
􏰞
Lobs(ψ|Dobs) =
p(Dobs, Dmis|ψ) dDmis. (12.41)
454 12. Cluster Analysis
TABLE 12.10. Plaid analysis of the leukemia data. Estimated column effects (μ􏰡 + β􏰡 ) for the first 8 layers. Samples whose estimated effects do
not appear in a column are not included in that layer.
j
￼Sample 1 2 3
4
5
6 7 8
￼￼ALLT 3 0.72 0.53 ALLB 4
ALLB 5 –1.04 1.15 –0.63
ALLT 6 0.66 ALLB 7 0.81
ALLB 8 1.09 0.74 ALLB 13 –0.86 1.10 –0.68
ALLT 14 0.61
ALLB 15 –1.19 1.07 –0.82
ALLB 16 0.63
ALLB 19 –0.51
ALLB 20 –1.24 1.39 –0.99
ALLB 21 –0.81 1.47
ALLB 22 0.65
ALLT 23
ALLB 24 0.96
ALLB 27 1.54 0.70 AML 28 –0.65 0.47 AML 29 –0.77
AML 30 –0.79
AML 31 –0.54
AML 32 –0.70 0.71 AML 33 0.86 –1.13 0.78 AML 34 0.69 –0.70 0.84 AML 35 1.06 –0.62
AML 36 –0.96 0.69 AML 37 –0.92 0.88 AML 38 0.67 –0.84
ALLB 39 0.72
ALLB 40 0.86
ALLB 41 –1.09 1.08 –0.63
ALLB 43
ALLB 44 –0.72 –0.43
ALLB 45 –0.74 –0.41
ALLB 46 –0.80 –0.47
ALLB 47 0.63 –0.60
ALLB 48 –0.74 1.25 –0.78
ALLB 49 1.29
AML 50 –0.85 0.93 AML 51 –0.85 0.97 AML 53 –0.94 0.77 ALLB 56 0.85 0.63 AML 58 1.04 –0.78 0.77 ALLB 59 –0.36
AML 61 0.71 –0.59
AML 62 –0.60
AML 63 –0.68
AML 64 1.06 –0.82 0.76 AML 65 –0.49
AML 66 –1.04
ALLB 68 –1.26 1.19 –0.74
ALLB 69 –1.04 0.90 –0.76
ALLB 70
0.63
0.84 1.37 1.58
0.49
1.54
0.67 –0.85
0.70 0.60
0.39 0.93 0.96
–0.69
1.07
1.31 0.85 0.68
–0.67
0.58
–0.71 –1.01 –0.83 –0.53
￼￼￼￼￼–0.96
￼￼￼￼–0.78 –1.25 –0.63 –0.75 –0.89
￼￼￼￼￼￼￼
12.9 Clustering Based Upon Mixture Models 455
TABLE 12.11. The EM algorithm.
1. Input: ψ􏰡(0) = initial guess for the parameter vector ψ.
2. Let D = {Dobs , Dmis } represent the “complete” data, where Dobs and Dmis are the portions of D which are observed and missing, respectively.
3. For m = 0, 1, 2, . . ., iterate between the following two steps: • E-step: Compute
􏰦􏰧 Q(ψ | ψ􏰡(m)) = E l(ψ|D) | Dobs,ψ􏰡(m)
as a function of ψ.
• M-step: Find ψ􏰡 (m+1) = arg maxψ Q(ψ | ψ􏰡 (m) ).
4. Stop when convergence of the log-likelihood is attained.
The MLE for ψ based upon the observed data Dobs is the ψ that maximizes Lobs(ψ|Dobs). Unfortunately, a direct attack on this problem usually fails.
The EM algorithm is tailor-made for this type of problem. It is a two- step iterative process, incorporating an expectation step (E-step) with a maximization step (M-step); see Table 12.11 for the algorithmic details. The E-step computes the conditional expectation of the complete-data log- likelihood given the observed data and the current parameter estimate, and the M-step updates the parameter estimate by maximizing the conditional expectation from the E-step.
Because p(Dmis|Dobs,ψ) = p(Dobs,Dmis|ψ)/p(Dobs|ψ), the observed- data log-likelihood is
l(ψ|Dobs) = log p(Dobs|ψ)
= l(ψ|D) − log p(Dmis|Dobs, ψ), (12.42)
where l(ψ|D) is the complete-data log-likelihood, which may be easy to compute, and logp(Dmis|Dobs,ψ) is the part of the complete-data log- likelihood due to the missing data. Taking expectations of (12.42) wrt the conditional density p(Dmis|Dobs,ψ′), where ψ′ is a current value of ψ, yields
￼￼where
l(ψ|Dobs) = Q(ψ|ψ′) − H(ψ|ψ′), 􏰞
Q(ψ|ψ′) = l(ψ|D)p(Dmis|Dobs, ψ′)dDmis = E{l(ψ|D)|Dobs , ψ′ },
(12.43)
(12.44)
456 12. Cluster Analysis
and
If we now set
then,
􏰞
log p(Dmis|Dobs, ψ)p(Dmis|Dobs, ψ′)dDmis E{log p(Dmis|Dobs, ψ)|Dobs, ψ′}.
h(Dmis)= p(Dmis|Dobs,ψ), p(Dmis|Dobs, ψ′)
H(ψ|ψ′) = =
(12.45) (12.46)
￼H(ψ|ψ′) − H(ψ′|ψ′) = E{log h(Dmis)|Dobs, ψ′} ≤ E{h(Dmis|Dobs, ψ′)} − 1
= 0,
where we used the inequality log x ≤ x − 1. Thus, H(ψ|ψ′) ≤ H(ψ′|ψ′).
From (12.43), the difference in l(ψ|Dobs) at the mth and (m + 1)st iter- ations is
l(ψ(m+1)|Dobs) − l(ψ(m)|Dobs)
≥ Q(ψ(m+1)|ψ(m)) − Q(ψ(m)|ψ(m)) ≥ 0, (12.48)
where we used (12.44) and the fact that the EM algorithm finds ψ(m+1) to make Q(ψ(m+1)|ψ(m)) > Q(ψ(m)|ψ(m)). Thus, the log-likelihood function
increases at each iteration (more accurately, it does not decrease). From this result, it can be shown that (under reasonably mild regularity con- ditions) convergence of the log-likelihood, at least to a local maximum, is ensured by this iterative process (Wu, 1983). Note, however, that local convergence of the log-likelihood does not automatically imply local con- vergence of the parameter estimates, although the latter convergence holds under additional regularity conditions.
The EM algorithm possesses reliable convergence properties and low cost per iteration, does not require much storage space, and is easy to program. Yet, it can be extremely slow to converge if there are many missing data and if the size of the data set is large. (We note that some effort has been made to speed up the EM algorithm.) Furthermore, because convergence is guaranteed only to a local maximum, and because likelihood surfaces often possess many local maxima, it is usually necessary to run the EM algorithm using different random starts to try to find a global maximum of the likelihood function.
12.9.1 The EM Algorithm for Finite Mixtures
One of the first applications of the EM algorithm was to the finite mix- tures problem. A density function p is a mixture of K component densities,
(12.47)
and
􏰇
1 if xi,obs ∈ Πk 0 otherwise
12.9 Clustering Based Upon Mixture Models 457
p1,...,pK, if p(x|{πk},{θk}) = 􏰊Kk=1 πkpk(x|θk), where {πk} are the mix- ingweights(0≤πk ≤1,k=1,2,...,K,􏰊Kk=1πk =1)andθk isavector of parameters for pk, which corresponds to the class Πk, k = 1,2,...,K.
Let Dobs = {x1,obs, · · · , xn,obs}, and define Dmis = {x1,mis, · · · , xn,mis}, where
xi,mis =(xi1,mis,···,xiK,mis)τ,
(12.49) (12.50)
xik,mis =
i = 1,2,...,n, k = 1,2,...,K. Thus, xi,mis is a K-vector that indicates whether the ith observation, xi,obs, is a member of Πk. Use xi,mis to aug- ment xi,obs to produce a “complete” data vector,
xi = (xτi,obs,xτi,mis)τ, i = 1,2,...,n. (12.51)
This idea of creating “missing data” for this problem as indicators of the unknown class labels was a key innovation of Dempster, Laird, and Rubin (1977).
Assume now that xi,mis is a realized value of the random vector Xi,mis = (Xi1,mis,···,XiK,mis)τ viewedasasingledrawfromaK-classmultinomial distribution with probabilities πk = P{Xi,obs ∈ Πk }, k = 1, 2, . . . , K . That is,
iid
Xi,mis ∼ MultK(1,π), i = 1,2,...,n,
where π = (π1,...,πK)τ. Hence,
􏰛K
Xi,obs|Xi,mis ∼ [pk(xi,obs|θk)]xik,mis.
(12.52)
(12.53)
k=1
Let ψ = {{πk},{θk}} represent all unknown parameters. The complete-
data log-likelihood is l(ψ|D) =
􏰏n 􏰏K i=1 k=1
xik,mis log{πkpk(xi,obs|θk)}, (12.54) where D = {x1,...,xn}. The E-step computes Q(ψ|ψ􏰡(m)) by replacing
each dummy variable xik,mis in (12.54) by its conditional expectation, x􏰡(m) = E{Xik,mis|Xi,obs,ψ􏰡(m)}, (12.55)
where ψ􏰡(m) is the current estimate of ψ. In other words, at the mth iter- ation, xik,mis is estimated by the posterior probability that Xi,obs ∈ Πk; from Section 8.5.1, this is
x􏰡(m) = ik,mis
ik,mis
π􏰡(m)p (x |θ􏰡(m))
k k i,obs k . (12.56)
􏰊K
j=1 j j i,obs j
￼π􏰡(m)p (x |θ􏰡(m))
458 12. Cluster Analysis
The M-step then takes the probabilities of class membership provided by the E-step, inserts them into (12.50) in place of xik,mis, and updates the parameter values from the E-step by maximizing (12.54) wrt {πk},{θk}. The M-step for the mixture proportions {πk} is given by
􏰏n i=1
The M-step for the parameter vector ψ depends upon the context. The E-step and M-step are iterated as many times as it is necessary to achieve convergence of the log-likelihood. The ML determination of the class of the ith observation is then the class corresponding to the largest value of x􏰡ik,mis, k = 1,2,...,K.
Consider, for example, a mixture of the two univariate Gaussian densities φ(x|θ1) and φ(x|θ2), where the parameter vectors are θ1 = (μ1,σ12)τ and θ2 = (μ2,σ2)τ, and the mixture proportions are π1 = 1−π and π2 = π. We also drop the subscript k. The E-step (12.56) reduces to
π􏰡(m+1) = n−1 k
x􏰡(m) , k = 1,2,...,K. (12.57) ik,mis
π􏰡 ( m ) φ ( x | θ􏰡 ( m ) )
x􏰡(m) = i,obs 2 , (12.58)
￼i,mis (1 − π􏰡(m))φ(xi,obs|θ􏰡(m)) + π􏰡(m)φ(xi,obs|θ􏰡(m)) 12
where π􏰡(m) = n−1 􏰊n x􏰡(m) i=1 i,mis
= x􏰡(m) ik,mis
. By maximizing (12.54) while fixing xik,mis , the M-step yields the estimates
􏰊n (1 − x􏰡(m) )xi,obs μ􏰡(m+1) = i=1 i,mis
(12.59)
(12.60)
(12.61)
(12.62)
,
(1 − x􏰡(m) )(xi,obs − μ􏰡(m+1))2
􏰊n (1−x􏰡(m) ) i=1 i,mis
￼( σ􏰡 12 ) ( m + 1 ) =
􏰊 i , m i s
n (1−x􏰡(m) )
1
􏰊n
i = 1
μ􏰡(m+1) = 2
i=1 i,mis
1 , 􏰊n x􏰡(m) xi,obs
￼i=1 i,mis
i=1 i,mis ,
􏰊n x􏰡(m) i=1 i,mis
￼􏰊n
(σ􏰡2)(m+1)= i=1 i,mis 2
x􏰡(m) (xi,obs − μ􏰡(m+1))2 2 􏰊n x􏰡(m)
.
￼Experimentation with this mixture model has shown that whereas conver- gence of the log-likelihood may be incredibly slow, most of the progress toward convergence tends to occur during the first few iterations (Redner and Walker, 1984).
In the multivariate Gaussian mixture problem (see Exercise 12.8), the “curse of dimensionality” raises its ugly head, where the number of param- eters grows quickly with the increase in dimensionality. Although PCA is
often used as a first step to reduce the dimensionality, this does not help in mixture problems because any class structure as exists may not be pre- served by the principal components (Chang, 1983). Furthermore, whenever estimates of the covariance matrix become singular or nearly singular, the EM algorithm breaks down; this can happen, for example, if the mixture has too many components and at least one of those components has too few observations, or when the dimensionality is greater than the number of observations, such as occurs with microarray experiments. This is currently an area of much research (Fraley and Raftery, 2002).
12.9.2 How Many Components?
The number of components, K, is one of the most important ingredients in mixture modeling, which becomes more complicated when the value of K is unknown. As a result, much attention has been paid to this issue. By and large, attempts at formulating test criteria to decide on the number of components have not been successful.
For example, an early decision procedure was the likelihood-ratio test statistic −2 log λk , where λk is the likelihood ratio (LR) (Wolfe, 1970). The LR compares a mixture having k components with a mixture having k + 1 components and then repeats the test for a succession of increasing values of k, each time comparing the result to a reference χ2-distribution. The testing stops the first time that a k-mixture density is not rejected in favor of a (k + 1)-mixture density. Recent empirical evidence indicates that this test tends to overestimate the value of K. More seriously, the regularity conditions for the χ2 approximation do not hold in finite-mixture problems.
Several alternatives to the likelihood ratio test have since been proposed. The two most prominent approaches are a nonparametric bootstrap assess- ment of the number of modes in the data using a kernel density estimator with a sequence of decreasing window-widths (Silverman, 1981, 1983) and a Bayesian solution that uses the EM algorithm to fit the mixture model and then computes approximate Bayes factors to decide on K (Fraley and Raftery, 2002). Silverman’s approach is promising, but there are a number of anomolies in its behavior (Izenman and Sommer, 1988). Bayes factors (Kass and Raftery, 1995) are ratios of high-dimensional integrals and are often impossible to compute; arguments have been made to justify BIC as approximate Bayes factors to estimate K, even though the regularity con- ditions for the BIC approximation do not hold for finite-mixture models.
12.10 Software Packages
Almost all the major statistical software packages contain hierarchical and non-hierarchical clustering routines for clustering observations or vari-
12.10 Software Packages 459
￼
460 12. Cluster Analysis
ables as appropriate. Software for two-way clustering methods, model-based clustering methods, and other recently developed methods have to be down- loaded from the Internet.
There are two SOM methods, batchSOM and SOM, in the R package (Ven- ables and Ripley, 2002, pp. 310–311) and a CRAN package som (formerly GeneSOM) for gene expression data. A SOM Toolbox for Matlab can be downloaded free from www.cis.hut.fi/projects/somtoolbox/.
Another package for computing SOMs is GeneCluster, which can be downloaded from the website www-genome.wi.mit.edu/cancer/software/software.html.
The U-matrix and component planes in Figures 12.11 and 12.12 were com- puted using Matlab somtoolbox.
A fast algorithm for gene-shaving forms the basis for the software package GeneClust, which can be downloaded free from odin.mdacc.tmc.edu/~kim/geneclust; see Do, Broom, and Wen (2003). Software and documentation (Owen, 2000) for applying plaid models to a data array can be downloaded from www-stat.stanford.edu/~owen/clickwrap/plaid.html.
Most research into model-based clustering from a Bayesian viewpoint has been carried out by Adrian Raftery and colleagues. Their S-Plus functions mclust and mclust-em and documentation (Fraley and Raftery, 1998) can be downloaded from www.stat.washington.edu/raftery/Research/Mclust.
The Emmix software package can fit a mixture model with Gaussian or t-components (McLachlan, Peel, Basford, and Abrams, 1999) and can be downloaded from www.jstatsoft.org.
Bibliographical Notes
Books that focus on cluster analysis include Kaufman and Rousseeuw (1990) and Hartigan (1975). Cluster analysis can be found as a chapter of most books on multivariate analysis: Rencher (2002, Chapter 14), Lattin, Carroll, and Green (2003, Chapter 8), Johnson and Wichern (1998, Chapter 12), Seber (1984, Chapter 7). See also Ripley (1996, Section 9.3).
Books on self-organizing maps include Oja and Kaski (2003), and Ko- honen (2001). There is also a Special Issue of Neural Networks in 2002 on New Developments in Self-Organizing Maps.
Review articles on the use of clustering in analyzing microarray data include Sebastiani, Gussoni, Kohane, and Ramoni (2003), Bryan (2004), and Chipman, Hastie, and Tibshirani (2003).
￼
There is a huge literature on mixtures of distributions. Book references include Everitt and Hand (1981), Titterington, Smith, and Makov (1985), McLachlan and Basford (1988), and McLachlan and Peel (2000). The idea of representing a density function as a mixture of two Gaussian components was popularized by Tukey (1960) as a way of modeling outliers in data, where he assumed equal means but different variances, one variance much larger than the other.
The EM algorithm has a long and interesting history, with the earliest version published in 1926. It was named in Dempster, Laird, and Rubin (1977), who showed the monotonic behavior of the log-likelihood function and gave examples of the general applicability of the algorithm. Books that give good accounts of the EM algorithm include McLachlan and Krish- nan (1996), Hastie, Tibshirani, and Friedman (2001, Section 8.5), Schafer (1997, Chapter 3), Ripley (1996, Appendix A.2), and Little and Rubin (1987, Chapter 7). See also the edited volume by Wanatabe and Yam- aguchi (2004). An excellent review of model-based clustering is given by Fraley and Raftery (2002).
Exercises
12.1 Run the clustering algorithms for the satimage data, but only using the center pixels (i.e., variables CC1, CC2, CC3, CC4) of each 3×3 neigh- borhood. Compare your results with those in Table 12.6.
12.2 Write a computer program to implement single-linkage, average- linkage, and complete-linkage agglomerative hierarchical clustering. Try it out on a data set of your choice.
12.3 Cluster the primate.scapulae data using single-linkage, average- linkage, and complete-linkage agglomerative hierarchical clustering meth- ods. Find the five-cluster solutions for all three methods, which allows com- parison with the true primate classifications. Find the misclassification rate for all three methods. Show that the lowest rate occurs for the complete- linkage method and the highest for the single-linkage method.
12.4 Using the leukemia (ALL/AML) data, run a SOM algorithm (ei- ther on-line or batch) to cluster the genes. Draw a SOM plot and identify the genes captured by each representative. Consult with a biologist to see whether the clusters of genes are biologically meaningful. Compute the U-matrix and the component planes. Solely on the basis of the patterns provided by the component planes, can you separate them into the three groups of ALL-B, ALL-T, and AML tissue samples?
12.5 Microarray data from the National Cancer Institute can be found in the file ncifinal.txt on the book’s website. There are 5,244 genes and 61
12.10 Exercises 461
￼
462 12. Cluster Analysis
samples in this data set; the samples are derived from tumors with different sites of origin: 7 breast, 5 central nervous system (CNS), 7 colon, 6 leukemia, 8 melanoma, 9 non–small-cell lung carcinoma (NSCLC), 6 ovarian, and 9 renal. There are also data from independent microarray experiments yield- ing 2 leukemia samples (K562) and 2 breast cancer samples (MCF7). Use the gene shaving method to cluster the genes in this data set into 8 clusters. Describe the appearance of the heatmap for each cluster, and use the gap statistic to determine the number of genes in each cluster.
12.6 Nutritional data from 961 different food items is given in the file food.txt, which can be downloaded from the book’s website or from http://www.ntwrks.com/~mikev/chart1.html. For each food item, there are 7 variables: fat (grams), food energy (calories), carbohydrates (grams), protein (grams), cholesterol (milligrams), weight (grams), and saturated fat (grams). To equalize out the different types of servings of each food, first divide each variable by weight of the food item. Next, because of the wide variations in the different variables, standardize each variable. The resulting data are X = (Xij). Apply plaid models to these data. Describe your findings for each of the first 10 layers.
12.7 Establish the ML estimates (12.57), (12.59)–(12.62) for the parame- ters of the two-component univariate Gaussian mixture.
12.8 Using the EM algorithm, find the ML estimates of the parameters of a finite mixture of multivariate Gaussian densities with equal covariance matrice Σ. Show that the ML estimate Σ􏰡(m) has to be inverted at each iteration m, which is one of the factors slowing down the computational speed of the algorithm.
12.9 Run a batch-SOM analysis on the Wisconsin Breast-Cancer data wbcd. Find the “circles” representation for the data and describe how well the SOM method clusters the tumor cases into benign and malignant. Compute the U-matrix and discuss its representation for these data.
